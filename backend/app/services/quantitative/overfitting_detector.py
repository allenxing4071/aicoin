"""
Promptè¿‡æ‹Ÿåˆæ£€æµ‹å™¨
é˜²æ­¢Promptåªé€‚åˆå†å²æ•°æ®ï¼Œæœªæ¥å¤±æ•ˆ
"""

import logging
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.models.prompt_template import PromptTemplate, PromptPerformance

logger = logging.getLogger(__name__)


class PromptOverfittingDetector:
    """
    Promptè¿‡æ‹Ÿåˆæ£€æµ‹å™¨
    
    æ ¸å¿ƒæ–¹æ³•ï¼š
    1. æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆWalk-Forward Analysisï¼‰
    2. æ ·æœ¬å†…/æ ·æœ¬å¤–è¡¨ç°å¯¹æ¯”
    3. ç¨³å®šæ€§è¯„åˆ†
    """
    
    def __init__(self, db: AsyncSession):
        self.db = db
    
    async def detect_overfitting(
        self,
        prompt_id: int,
        in_sample_returns: List[float],
        out_sample_returns: List[float]
    ) -> Dict[str, any]:
        """
        æ£€æµ‹Promptæ˜¯å¦è¿‡æ‹Ÿåˆ
        
        Args:
            prompt_id: Prompt ID
            in_sample_returns: æ ·æœ¬å†…æ”¶ç›Šç‡åºåˆ—ï¼ˆè®­ç»ƒé›†ï¼‰
            out_sample_returns: æ ·æœ¬å¤–æ”¶ç›Šç‡åºåˆ—ï¼ˆæµ‹è¯•é›†ï¼‰
        
        Returns:
            {
                "overfitting_score": 0.75,  # 0-1ï¼Œè¶Šé«˜è¶Šå¯èƒ½è¿‡æ‹Ÿåˆ
                "in_sample_win_rate": 0.72,
                "out_sample_win_rate": 0.58,
                "performance_degradation": 0.14,  # æ€§èƒ½ä¸‹é™å¹…åº¦
                "stability_score": 0.45,
                "warning": "é«˜é£é™©è¿‡æ‹Ÿåˆ"
            }
        """
        if not in_sample_returns or not out_sample_returns:
            return {"overfitting_score": 0, "warning": "æ ·æœ¬ä¸è¶³"}
        
        # 1. è®¡ç®—æ ·æœ¬å†…è¡¨ç°
        in_sample_win_rate = self._calculate_win_rate(in_sample_returns)
        in_sample_sharpe = self._calculate_sharpe(in_sample_returns)
        
        # 2. è®¡ç®—æ ·æœ¬å¤–è¡¨ç°
        out_sample_win_rate = self._calculate_win_rate(out_sample_returns)
        out_sample_sharpe = self._calculate_sharpe(out_sample_returns)
        
        # 3. è®¡ç®—æ€§èƒ½ä¸‹é™å¹…åº¦
        win_rate_degradation = in_sample_win_rate - out_sample_win_rate
        sharpe_degradation = (in_sample_sharpe - out_sample_sharpe) / max(in_sample_sharpe, 0.01)
        
        # 4. è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
        stability_score = self._calculate_stability(in_sample_returns, out_sample_returns)
        
        # 5. ç»¼åˆè¿‡æ‹Ÿåˆè¯„åˆ†
        overfitting_score = self._calculate_overfitting_score(
            win_rate_degradation,
            sharpe_degradation,
            stability_score
        )
        
        # 6. ç”Ÿæˆè­¦å‘Š
        warning = self._generate_warning(overfitting_score)
        
        result = {
            "overfitting_score": round(overfitting_score, 2),
            "in_sample_win_rate": round(in_sample_win_rate, 4),
            "out_sample_win_rate": round(out_sample_win_rate, 4),
            "in_sample_sharpe": round(in_sample_sharpe, 2),
            "out_sample_sharpe": round(out_sample_sharpe, 2),
            "performance_degradation": round(win_rate_degradation, 4),
            "stability_score": round(stability_score, 2),
            "warning": warning
        }
        
        logger.info(f"Prompt {prompt_id} è¿‡æ‹Ÿåˆæ£€æµ‹: score={overfitting_score:.2f}, {warning}")
        
        return result
    
    def _calculate_win_rate(self, returns: List[float]) -> float:
        """è®¡ç®—èƒœç‡"""
        if not returns:
            return 0.0
        wins = sum(1 for r in returns if r > 0)
        return wins / len(returns)
    
    def _calculate_sharpe(self, returns: List[float]) -> float:
        """è®¡ç®—å¤æ™®æ¯”ç‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not returns or len(returns) < 2:
            return 0.0
        
        returns_array = np.array(returns)
        mean_return = np.mean(returns_array)
        std_return = np.std(returns_array, ddof=1)
        
        if std_return == 0:
            return 0.0
        
        return mean_return / std_return * np.sqrt(252)  # å¹´åŒ–
    
    def _calculate_stability(
        self,
        in_sample_returns: List[float],
        out_sample_returns: List[float]
    ) -> float:
        """
        è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
        
        æ–¹æ³•ï¼šæ¯”è¾ƒæ ·æœ¬å†…å’Œæ ·æœ¬å¤–çš„æ”¶ç›Šç‡åˆ†å¸ƒç›¸ä¼¼åº¦
        
        Returns:
            0-1ä¹‹é—´ï¼Œè¶Šé«˜è¶Šç¨³å®š
        """
        if not in_sample_returns or not out_sample_returns:
            return 0.0
        
        # è®¡ç®—ä¸¤ç»„æ•°æ®çš„ç»Ÿè®¡ç‰¹å¾
        in_mean = np.mean(in_sample_returns)
        in_std = np.std(in_sample_returns)
        out_mean = np.mean(out_sample_returns)
        out_std = np.std(out_sample_returns)
        
        # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®çš„ç›¸å¯¹å·®å¼‚
        mean_diff = abs(in_mean - out_mean) / max(abs(in_mean), 0.01)
        std_diff = abs(in_std - out_std) / max(in_std, 0.01)
        
        # ç¨³å®šæ€§è¯„åˆ†ï¼ˆå·®å¼‚è¶Šå°ï¼Œç¨³å®šæ€§è¶Šé«˜ï¼‰
        stability = 1.0 - min((mean_diff + std_diff) / 2, 1.0)
        
        return stability
    
    def _calculate_overfitting_score(
        self,
        win_rate_degradation: float,
        sharpe_degradation: float,
        stability_score: float
    ) -> float:
        """
        è®¡ç®—ç»¼åˆè¿‡æ‹Ÿåˆè¯„åˆ†
        
        å…¬å¼ï¼š
        overfitting_score = 0.4 * win_rate_deg + 0.3 * sharpe_deg + 0.3 * (1 - stability)
        
        Returns:
            0-1ä¹‹é—´ï¼Œè¶Šé«˜è¶Šå¯èƒ½è¿‡æ‹Ÿåˆ
        """
        # å½’ä¸€åŒ–
        win_rate_deg_norm = min(max(win_rate_degradation, 0), 1)
        sharpe_deg_norm = min(max(sharpe_degradation, 0), 1)
        instability = 1.0 - stability_score
        
        # åŠ æƒæ±‚å’Œ
        score = (
            0.4 * win_rate_deg_norm +
            0.3 * sharpe_deg_norm +
            0.3 * instability
        )
        
        return min(score, 1.0)
    
    def _generate_warning(self, overfitting_score: float) -> str:
        """ç”Ÿæˆè­¦å‘Šä¿¡æ¯"""
        if overfitting_score < 0.3:
            return "âœ… ä½é£é™©ï¼ŒPromptè¡¨ç°ç¨³å®š"
        elif overfitting_score < 0.5:
            return "âš ï¸  ä¸­ç­‰é£é™©ï¼Œå»ºè®®ç›‘æ§"
        elif overfitting_score < 0.7:
            return "âš ï¸  é«˜é£é™©è¿‡æ‹Ÿåˆï¼Œå»ºè®®ä¼˜åŒ–"
        else:
            return "ğŸš¨ æé«˜é£é™©è¿‡æ‹Ÿåˆï¼Œå»ºè®®ç«‹å³åœç”¨"
    
    async def walk_forward_analysis(
        self,
        prompt_id: int,
        all_returns: List[Tuple[datetime, float]],
        train_window_days: int = 60,
        test_window_days: int = 30
    ) -> Dict[str, any]:
        """
        æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼ˆWalk-Forward Analysisï¼‰
        
        æ–¹æ³•ï¼š
        1. å°†æ•°æ®åˆ†ä¸ºå¤šä¸ªæ—¶é—´çª—å£
        2. æ¯ä¸ªçª—å£ï¼šå‰60å¤©è®­ç»ƒï¼Œå30å¤©æµ‹è¯•
        3. æ»šåŠ¨å‰è¿›ï¼Œé‡å¤éªŒè¯
        
        Args:
            prompt_id: Prompt ID
            all_returns: [(æ—¶é—´æˆ³, æ”¶ç›Šç‡), ...]
            train_window_days: è®­ç»ƒçª—å£å¤©æ•°
            test_window_days: æµ‹è¯•çª—å£å¤©æ•°
        
        Returns:
            {
                "num_windows": 5,
                "avg_overfitting_score": 0.45,
                "windows": [...]
            }
        """
        if not all_returns or len(all_returns) < train_window_days + test_window_days:
            return {"error": "æ•°æ®ä¸è¶³"}
        
        # æŒ‰æ—¶é—´æ’åº
        sorted_returns = sorted(all_returns, key=lambda x: x[0])
        
        windows = []
        window_size = train_window_days + test_window_days
        
        # æ»šåŠ¨çª—å£
        for i in range(0, len(sorted_returns) - window_size + 1, test_window_days):
            # è®­ç»ƒé›†
            train_start = i
            train_end = i + train_window_days
            train_returns = [r[1] for r in sorted_returns[train_start:train_end]]
            
            # æµ‹è¯•é›†
            test_start = train_end
            test_end = test_start + test_window_days
            test_returns = [r[1] for r in sorted_returns[test_start:test_end]]
            
            # æ£€æµ‹è¿‡æ‹Ÿåˆ
            result = await self.detect_overfitting(prompt_id, train_returns, test_returns)
            
            windows.append({
                "window_id": len(windows) + 1,
                "train_period": f"{sorted_returns[train_start][0].date()} ~ {sorted_returns[train_end-1][0].date()}",
                "test_period": f"{sorted_returns[test_start][0].date()} ~ {sorted_returns[test_end-1][0].date()}",
                "overfitting_score": result["overfitting_score"],
                "in_sample_win_rate": result["in_sample_win_rate"],
                "out_sample_win_rate": result["out_sample_win_rate"]
            })
        
        # è®¡ç®—å¹³å‡è¿‡æ‹Ÿåˆè¯„åˆ†
        avg_score = np.mean([w["overfitting_score"] for w in windows])
        
        logger.info(f"Walk-Forwardåˆ†æå®Œæˆ: {len(windows)}ä¸ªçª—å£, å¹³å‡è¿‡æ‹Ÿåˆè¯„åˆ†={avg_score:.2f}")
        
        return {
            "num_windows": len(windows),
            "avg_overfitting_score": round(avg_score, 2),
            "windows": windows
        }
    
    async def monitor_out_of_sample_performance(
        self,
        prompt_id: int,
        threshold: float = 0.10
    ) -> Dict[str, any]:
        """
        ç›‘æ§æ ·æœ¬å¤–è¡¨ç°
        
        æ–¹æ³•ï¼š
        1. è·å–Promptçš„å†å²è¡¨ç°ï¼ˆæ ·æœ¬å†…ï¼‰
        2. è·å–æœ€è¿‘30å¤©çš„è¡¨ç°ï¼ˆæ ·æœ¬å¤–ï¼‰
        3. å¯¹æ¯”å·®å¼‚
        
        Args:
            prompt_id: Prompt ID
            threshold: è­¦å‘Šé˜ˆå€¼ï¼ˆå¦‚0.10è¡¨ç¤ºèƒœç‡ä¸‹é™10%è§¦å‘è­¦å‘Šï¼‰
        
        Returns:
            {
                "historical_win_rate": 0.65,
                "recent_win_rate": 0.52,
                "degradation": 0.13,
                "alert": True
            }
        """
        # TODO: ä»æ•°æ®åº“æŸ¥è¯¢å†å²æ•°æ®
        # è¿™é‡Œç®€åŒ–ä¸ºç¤ºä¾‹
        
        return {
            "historical_win_rate": 0.65,
            "recent_win_rate": 0.52,
            "degradation": 0.13,
            "alert": True,
            "message": "æ ·æœ¬å¤–è¡¨ç°æ˜¾è‘—ä¸‹é™ï¼Œå»ºè®®é‡æ–°è¯„ä¼°Prompt"
        }


"""Intelligence Coordinator - ç»Ÿä¸€æƒ…æŠ¥åè°ƒå™¨

æ•´åˆæ‰€æœ‰æƒ…æŠ¥åŠŸèƒ½ï¼š
1. åè°ƒå¤šå¹³å°æƒ…æŠ¥æ”¶é›†ï¼ˆMultiPlatformCoordinatorï¼‰
2. ç®¡ç†å››å±‚å­˜å‚¨æµè½¬ï¼ˆL1-L4ï¼‰
3. æä¾›ç»Ÿä¸€çš„æƒ…æŠ¥æ¥å£
4. æ”¯æŒé…ç½®åŒ–å¼€å…³
"""

import logging
import asyncio
import time
from typing import Dict, Any, Optional
from datetime import datetime

from app.core.config import settings
from app.utils.timezone import get_beijing_time
from .models import IntelligenceReport, SentimentType, NewsItem, WhaleActivity, OnChainMetrics
from .qwen_engine import QwenIntelligenceEngine
from .multi_platform_coordinator import MultiPlatformCoordinator
from .storage_layers import (
    ShortTermIntelligenceCache,
    MidTermIntelligenceAnalyzer,
    LongTermIntelligenceStore,
    IntelligenceVectorKB
)
from .platforms import (
    FreePlatformAdapter,
    QwenSearchAdapter,
    QwenDeepAdapter
)
from .data_sources import crypto_news_api, on_chain_data_api

logger = logging.getLogger(__name__)


class IntelligenceCoordinator:
    """
    ç»Ÿä¸€æƒ…æŠ¥åè°ƒå™¨ - æ•´åˆæ‰€æœ‰æƒ…æŠ¥åŠŸèƒ½
    
    èŒè´£ï¼š
    1. åè°ƒå¤šå¹³å°æƒ…æŠ¥æ”¶é›†ï¼ˆMultiPlatformCoordinatorï¼‰
    2. ç®¡ç†å››å±‚å­˜å‚¨æµè½¬ï¼ˆL1-L4ï¼‰
    3. æä¾›ç»Ÿä¸€çš„æƒ…æŠ¥æ¥å£
    4. æ”¯æŒé…ç½®åŒ–å¼€å…³
    """
    
    def __init__(self, redis_client, db_session):
        """
        åˆå§‹åŒ–ç»Ÿä¸€æƒ…æŠ¥åè°ƒå™¨
        
        Args:
            redis_client: Rediså®¢æˆ·ç«¯
            db_session: æ•°æ®åº“ä¼šè¯
        """
        self.redis_client = redis_client
        self.db_session = db_session
        
        # é…ç½®å¼€å…³
        self.use_multi_platform = getattr(settings, 'INTELLIGENCE_USE_MULTI_PLATFORM', True)
        self.use_storage_layers = getattr(settings, 'INTELLIGENCE_USE_STORAGE_LAYERS', True)
        
        # å¼‚æ­¥ä»»åŠ¡è¿½è¸ª
        self._storage_tasks: list = []
        self._task_lock = asyncio.Lock()
        
        # åˆå§‹åŒ–å››å±‚å­˜å‚¨
        try:
            self.l1_cache = ShortTermIntelligenceCache(redis_client)
            self.l2_analyzer = MidTermIntelligenceAnalyzer(redis_client, db_session)
            self.l3_store = LongTermIntelligenceStore(db_session)
            self.l4_vector = IntelligenceVectorKB(
                qdrant_host=getattr(settings, 'QDRANT_HOST', 'localhost'),
                qdrant_port=getattr(settings, 'QDRANT_PORT', 6333),
                collection_name="intelligence_knowledge",
                embedding_provider="qwen"
            )
            logger.info("âœ… å››å±‚å­˜å‚¨æ¶æ„åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âš ï¸ å››å±‚å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ç¦ç”¨å­˜å‚¨å±‚åŠŸèƒ½")
            self.use_storage_layers = False
            self.l1_cache = None
            self.l2_analyzer = None
            self.l3_store = None
            self.l4_vector = None
        
        # åˆå§‹åŒ–å¤šå¹³å°åè°ƒå™¨
        try:
            self.multi_platform = MultiPlatformCoordinator(
                free_platform=FreePlatformAdapter(),
                search_platform=QwenSearchAdapter(),
                deep_platform=QwenDeepAdapter()
            )
            logger.info("âœ… å¤šå¹³å°åè°ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âš ï¸ å¤šå¹³å°åè°ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨fallbackå¼•æ“")
            self.use_multi_platform = False
            self.multi_platform = None
        
        # ä¿ç•™æ—§ç‰ˆå¼•æ“ä½œä¸ºfallback
        self.fallback_engine = QwenIntelligenceEngine()
        
        logger.info(f"âœ… IntelligenceCoordinatoråˆå§‹åŒ–å®Œæˆ (å¤šå¹³å°={self.use_multi_platform}, å­˜å‚¨å±‚={self.use_storage_layers})")
    
    async def collect_intelligence(self) -> IntelligenceReport:
        """
        ç»Ÿä¸€çš„æƒ…æŠ¥æ”¶é›†å…¥å£
        
        Returns:
            IntelligenceReport: æƒ…æŠ¥æŠ¥å‘Š
        """
        start_time = time.time()
        
        try:
            logger.info("ğŸ•µï¸â€â™€ï¸ å¼€å§‹æƒ…æŠ¥æ”¶é›†...")
            
            # é€‰æ‹©æƒ…æŠ¥æ”¶é›†ç­–ç•¥
            if self.use_multi_platform and self.multi_platform:
                logger.info("ğŸ“Š ä½¿ç”¨å¤šå¹³å°åè°ƒç­–ç•¥")
                report = await self._collect_with_multi_platform()
            else:
                logger.info("ğŸ“Š ä½¿ç”¨fallbackå¼•æ“")
                report = await self.fallback_engine.collect_intelligence()
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            duration = time.time() - start_time
            logger.info(f"âœ… æƒ…æŠ¥æ”¶é›†å®Œæˆï¼Œè€—æ—¶: {duration:.2f}ç§’")
            
            # å­˜å‚¨åˆ°å››å±‚æ¶æ„ï¼ˆå¼‚æ­¥ï¼Œå¸¦è¿½è¸ªå’Œé”™è¯¯å¤„ç†ï¼‰
            if self.use_storage_layers and report:
                task = asyncio.create_task(self._store_to_layers_with_tracking(report))
                async with self._task_lock:
                    self._storage_tasks.append(task)
                    # æ¸…ç†å·²å®Œæˆçš„ä»»åŠ¡
                    self._storage_tasks = [t for t in self._storage_tasks if not t.done()]
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ æƒ…æŠ¥æ”¶é›†å¤±è´¥: {e}ï¼Œä½¿ç”¨fallbackå¼•æ“", exc_info=True)
            try:
                return await self.fallback_engine.collect_intelligence()
            except Exception as fallback_error:
                logger.error(f"âŒ Fallbackå¼•æ“ä¹Ÿå¤±è´¥: {fallback_error}")
                return self._create_emergency_report()
    
    async def _collect_with_multi_platform(self) -> IntelligenceReport:
        """
        ä½¿ç”¨å¤šå¹³å°åè°ƒå™¨æ”¶é›†æƒ…æŠ¥
        
        Returns:
            IntelligenceReport: å¢å¼ºçš„æƒ…æŠ¥æŠ¥å‘Š
        """
        try:
            # å‡†å¤‡æ•°æ®æº
            logger.info("ğŸ“¡ æ”¶é›†åŸå§‹æ•°æ®æº...")
            news_items = await crypto_news_api.fetch_latest_news(limit=10)
            whale_signals = await on_chain_data_api.detect_whale_activity()
            on_chain_metrics = await on_chain_data_api.fetch_on_chain_metrics()
            
            data_sources = {
                "news": news_items,
                "whale": whale_signals,
                "onchain": on_chain_metrics
            }
            
            logger.info(f"âœ… æ•°æ®æºæ”¶é›†å®Œæˆ: {len(news_items)}æ¡æ–°é—», {len(whale_signals)}ä¸ªå·¨é²¸ä¿¡å·")
            
            # å¤šå¹³å°åè°ƒåˆ†æ
            logger.info("ğŸ”„ å¯åŠ¨å¤šå¹³å°åè°ƒåˆ†æ...")
            result = await self.multi_platform.coordinate_analysis(
                data_sources=data_sources,
                query_context={"require_realtime": True}
            )
            
            # è½¬æ¢ä¸ºIntelligenceReportæ ¼å¼
            report = self._convert_to_report(result, news_items, whale_signals, on_chain_metrics)
            
            logger.info(f"âœ… å¤šå¹³å°åè°ƒå®Œæˆ: æƒ…ç»ª={report.market_sentiment.value}, ç½®ä¿¡åº¦={report.confidence:.2f}")
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ å¤šå¹³å°åè°ƒå¤±è´¥: {e}", exc_info=True)
            raise
    
    def _convert_to_report(
        self,
        multi_platform_result: Dict[str, Any],
        news_items: list,
        whale_signals: list,
        on_chain_metrics: Any
    ) -> IntelligenceReport:
        """
        å°†å¤šå¹³å°åè°ƒç»“æœè½¬æ¢ä¸ºIntelligenceReportæ ¼å¼
        
        Args:
            multi_platform_result: å¤šå¹³å°åè°ƒç»“æœ
            news_items: æ–°é—»åˆ—è¡¨
            whale_signals: å·¨é²¸ä¿¡å·åˆ—è¡¨
            on_chain_metrics: é“¾ä¸ŠæŒ‡æ ‡
        
        Returns:
            IntelligenceReport: å¢å¼ºçš„æƒ…æŠ¥æŠ¥å‘Š
        """
        # è§£æå¸‚åœºæƒ…ç»ª
        sentiment_str = multi_platform_result.get("market_sentiment", "neutral").upper()
        try:
            sentiment = SentimentType[sentiment_str]
        except KeyError:
            sentiment = SentimentType.NEUTRAL
        
        # åˆ›å»ºæŠ¥å‘Š
        report = IntelligenceReport(
            timestamp=get_beijing_time(),
            market_sentiment=sentiment,
            sentiment_score=multi_platform_result.get("sentiment_score", 0.0),
            key_news=news_items[:5],
            whale_signals=whale_signals,
            on_chain_metrics=on_chain_metrics,
            risk_factors=multi_platform_result.get("risk_factors", []),
            opportunities=multi_platform_result.get("opportunities", []),
            qwen_analysis=multi_platform_result.get("analysis", ""),
            confidence=multi_platform_result.get("confidence", 0.7)
        )
        
        # æ·»åŠ å¤šå¹³å°éªŒè¯ä¿¡æ¯ï¼ˆæ‰©å±•å±æ€§ï¼‰
        report.platform_contributions = multi_platform_result.get("platform_contributions", {})
        report.platform_consensus = multi_platform_result.get("coordination_metadata", {}).get("platform_consensus", 0.0)
        report.verification_metadata = multi_platform_result.get("coordination_metadata", {})
        report.summary = multi_platform_result.get("analysis", "")[:500]
        
        return report
    
    async def _store_to_layers(self, report: IntelligenceReport):
        """
        å­˜å‚¨åˆ°å››å±‚æ¶æ„ï¼ˆå¼‚æ­¥æ‰§è¡Œï¼‰
        
        Args:
            report: æƒ…æŠ¥æŠ¥å‘Š
        """
        try:
            report_id = f"intel_{report.timestamp.strftime('%Y%m%d_%H%M%S')}"
            report_data = self._report_to_dict(report)
            
            # L1: çŸ­æœŸç¼“å­˜ï¼ˆç«‹å³æ‰§è¡Œï¼‰
            if self.l1_cache:
                await self.l1_cache.store_report(report_id, report_data)
                logger.info(f"âœ… L1ç¼“å­˜å®Œæˆ: {report_id}")
            
            # L2: è§¦å‘ä¸­æœŸåˆ†æï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡ï¼‰
            if self.l2_analyzer:
                asyncio.create_task(self._trigger_l2_analysis())
            
            # L3: é•¿æœŸå­˜å‚¨ï¼ˆå¼‚æ­¥ï¼‰
            if self.l3_store:
                asyncio.create_task(self._store_to_l3(report))
            
            # L4: å‘é‡åŒ–ï¼ˆå¼‚æ­¥ï¼‰
            if self.l4_vector:
                asyncio.create_task(self._vectorize_to_l4(report, report_id))
            
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨åˆ°å››å±‚æ¶æ„å¤±è´¥: {e}", exc_info=True)
    
    async def _store_to_layers_with_tracking(self, report: IntelligenceReport):
        """
        å­˜å‚¨åˆ°å››å±‚æ¶æ„ï¼ˆå¸¦é”™è¯¯è¿½è¸ªå’Œé‡è¯•ï¼‰
        
        Args:
            report: æƒ…æŠ¥æŠ¥å‘Š
        """
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                await self._store_to_layers(report)
                logger.info(f"âœ… å››å±‚å­˜å‚¨æˆåŠŸ")
                return
            except Exception as e:
                retry_count += 1
                logger.error(f"âŒ å››å±‚å­˜å‚¨å¤±è´¥ (å°è¯• {retry_count}/{max_retries}): {e}")
                if retry_count < max_retries:
                    await asyncio.sleep(2 ** retry_count)  # æŒ‡æ•°é€€é¿
                else:
                    logger.error(f"âŒ å››å±‚å­˜å‚¨æœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
    
    async def wait_for_storage_tasks(self, timeout: float = 30.0):
        """
        ç­‰å¾…æ‰€æœ‰å­˜å‚¨ä»»åŠ¡å®Œæˆ
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        if not self._storage_tasks:
            return
        
        try:
            async with self._task_lock:
                tasks = [t for t in self._storage_tasks if not t.done()]
            
            if tasks:
                logger.info(f"â³ ç­‰å¾… {len(tasks)} ä¸ªå­˜å‚¨ä»»åŠ¡å®Œæˆ...")
                await asyncio.wait_for(
                    asyncio.gather(*tasks, return_exceptions=True),
                    timeout=timeout
                )
                logger.info(f"âœ… æ‰€æœ‰å­˜å‚¨ä»»åŠ¡å·²å®Œæˆ")
        except asyncio.TimeoutError:
            logger.warning(f"âš ï¸ å­˜å‚¨ä»»åŠ¡ç­‰å¾…è¶…æ—¶ ({timeout}ç§’)")
        except Exception as e:
            logger.error(f"âŒ ç­‰å¾…å­˜å‚¨ä»»åŠ¡å¤±è´¥: {e}")

    async def _trigger_l2_analysis(self):
        """è§¦å‘L2ä¸­æœŸåˆ†æ"""
        try:
            logger.info("ğŸ” å¼€å§‹L2ä¸­æœŸåˆ†æ...")
            
            # åˆ†æç”¨æˆ·è¡Œä¸º
            behavior = await self.l2_analyzer.analyze_user_behavior(time_window_hours=24)
            
            # è®¡ç®—ä¿¡æ¯æºæƒé‡
            weights = await self.l2_analyzer.calculate_source_weights()
            
            # è¯†åˆ«é«˜ä»·å€¼æ¨¡å¼
            patterns = await self.l2_analyzer.identify_high_value_patterns()
            
            # å‡†å¤‡å‘é‡åŒ–å€™é€‰
            candidates = await self.l2_analyzer.prepare_vectorization_candidates(
                min_interaction_threshold=3
            )
            
            logger.info(f"âœ… L2åˆ†æå®Œæˆ: {len(weights)}ä¸ªæº, {len(patterns)}ä¸ªæ¨¡å¼, {len(candidates)}ä¸ªå€™é€‰")
            
        except Exception as e:
            logger.error(f"âŒ L2åˆ†æå¤±è´¥: {e}", exc_info=True)
    
    async def _store_to_l3(self, report: IntelligenceReport):
        """å­˜å‚¨åˆ°L3é•¿æœŸå­˜å‚¨"""
        try:
            # å­˜å‚¨ä¿¡æ¯æºæƒé‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(report, 'platform_contributions'):
                for platform, contribution in report.platform_contributions.items():
                    await self.l3_store.store_source_weight(
                        source_name=platform,
                        source_type="platform",
                        weight=contribution.get('confidence', 0.5),
                        metrics={
                            'usage_count': 1,
                            'positive_feedback': 0,
                            'effectiveness': contribution.get('confidence', 0.5)
                        }
                    )
            
            logger.info("âœ… L3å­˜å‚¨å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ L3å­˜å‚¨å¤±è´¥: {e}", exc_info=True)
    
    async def _vectorize_to_l4(self, report: IntelligenceReport, report_id: str):
        """å‘é‡åŒ–åˆ°L4çŸ¥è¯†åº“"""
        try:
            # æ„å»ºå‘é‡åŒ–å†…å®¹
            content = f"{report.qwen_analysis}\n\n"
            content += f"å¸‚åœºæƒ…ç»ª: {report.market_sentiment.value}\n"
            content += f"é£é™©å› ç´ : {', '.join(report.risk_factors[:3])}\n"
            content += f"æœºä¼šç‚¹: {', '.join(report.opportunities[:2])}"
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "source": "multi_platform",
                "category": "market_intelligence",
                "sentiment": report.market_sentiment.value,
                "importance": report.confidence,
                "timestamp": report.timestamp
            }
            
            # å‘é‡åŒ–
            await self.l4_vector.vectorize_intelligence(
                intelligence_id=report_id,
                content=content,
                metadata=metadata
            )
            
            logger.info("âœ… L4å‘é‡åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ L4å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)
    
    def _report_to_dict(self, report: IntelligenceReport) -> Dict[str, Any]:
        """å°†IntelligenceReportè½¬æ¢ä¸ºå­—å…¸"""
        data = {
            "timestamp": report.timestamp.isoformat(),
            "market_sentiment": report.market_sentiment.value,
            "sentiment_score": report.sentiment_score,
            "confidence": report.confidence,
            "qwen_analysis": report.qwen_analysis,
            "risk_factors": report.risk_factors,
            "opportunities": report.opportunities,
            "key_news": [
                {
                    "title": news.title,
                    "source": news.source,
                    "url": news.url,
                    "published_at": news.published_at.isoformat() if hasattr(news.published_at, 'isoformat') else str(news.published_at),
                    "impact": news.impact,
                    "sentiment": news.sentiment
                }
                for news in report.key_news
            ] if report.key_news else [],
            "whale_signals": [
                {
                    "symbol": whale.symbol,
                    "action": whale.action,
                    "amount_usd": whale.amount_usd,
                    "timestamp": whale.timestamp.isoformat() if hasattr(whale.timestamp, 'isoformat') else str(whale.timestamp)
                }
                for whale in report.whale_signals
            ] if report.whale_signals else []
        }
        
        # æ·»åŠ æ‰©å±•å±æ€§
        if hasattr(report, 'platform_contributions'):
            data['platform_contributions'] = report.platform_contributions
        if hasattr(report, 'platform_consensus'):
            data['platform_consensus'] = report.platform_consensus
        if hasattr(report, 'verification_metadata'):
            data['verification_metadata'] = report.verification_metadata
        if hasattr(report, 'summary'):
            data['summary'] = report.summary
        
        return data
    
    def _create_emergency_report(self) -> IntelligenceReport:
        """åˆ›å»ºç´§æ€¥fallbackæŠ¥å‘Šï¼ˆå½“æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥æ—¶ï¼‰"""
        return IntelligenceReport(
            timestamp=get_beijing_time(),
            market_sentiment=SentimentType.NEUTRAL,
            sentiment_score=0.0,
            key_news=[],
            whale_signals=[],
            on_chain_metrics=OnChainMetrics(
                exchange_net_flow=0,
                active_addresses=0,
                gas_price=0,
                transaction_volume=0,
                timestamp=get_beijing_time()
            ),
            risk_factors=["æƒ…æŠ¥ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨"],
            opportunities=[],
            qwen_analysis="æƒ…æŠ¥æ”¶é›†ç³»ç»Ÿæš‚æ—¶ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®ã€‚",
            confidence=0.3
        )
    
    async def get_latest_intelligence(self) -> Optional[IntelligenceReport]:
        """
        ä»L1ç¼“å­˜è·å–æœ€æ–°æƒ…æŠ¥ï¼ˆå¿«é€Ÿè®¿é—®ï¼‰
        
        Returns:
            Optional[IntelligenceReport]: æœ€æ–°æƒ…æŠ¥æŠ¥å‘Š
        """
        try:
            if self.use_storage_layers and self.l1_cache:
                cached_data = await self.l1_cache.get_latest_report()
                if cached_data:
                    return self._dict_to_report(cached_data)
            
            # Fallback: ä»æ—§å­˜å‚¨è·å–
            from .storage import intelligence_storage
            return await intelligence_storage.get_latest_report()
            
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–æœ€æ–°æƒ…æŠ¥å¤±è´¥: {e}")
            return None
    
    def _dict_to_report(self, data: Dict[str, Any]) -> IntelligenceReport:
        """å°†å­—å…¸è½¬æ¢ä¸ºIntelligenceReport"""
        # è§£ææ—¶é—´æˆ³
        timestamp_str = data.get("timestamp")
        if isinstance(timestamp_str, str):
            timestamp = datetime.fromisoformat(timestamp_str)
        else:
            timestamp = get_beijing_time()
        
        # è§£æå¸‚åœºæƒ…ç»ª
        sentiment_str = data.get("market_sentiment", "NEUTRAL")
        try:
            sentiment = SentimentType[sentiment_str]
        except KeyError:
            sentiment = SentimentType.NEUTRAL
        
        # åˆ›å»ºæŠ¥å‘Š
        report = IntelligenceReport(
            timestamp=timestamp,
            market_sentiment=sentiment,
            sentiment_score=data.get("sentiment_score", 0.0),
            key_news=[],  # ç®€åŒ–å¤„ç†
            whale_signals=[],  # ç®€åŒ–å¤„ç†
            on_chain_metrics=OnChainMetrics(
                exchange_net_flow=0,
                active_addresses=0,
                gas_price=0,
                transaction_volume=0,
                timestamp=timestamp
            ),
            risk_factors=data.get("risk_factors", []),
            opportunities=data.get("opportunities", []),
            qwen_analysis=data.get("qwen_analysis", ""),
            confidence=data.get("confidence", 0.7)
        )
        
        # æ·»åŠ æ‰©å±•å±æ€§
        if 'platform_contributions' in data:
            report.platform_contributions = data['platform_contributions']
        if 'platform_consensus' in data:
            report.platform_consensus = data['platform_consensus']
        if 'verification_metadata' in data:
            report.verification_metadata = data['verification_metadata']
        if 'summary' in data:
            report.summary = data['summary']
        
        return report


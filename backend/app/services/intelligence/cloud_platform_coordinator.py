"""Cloud Platform Coordinator - äº‘å¹³å°å¹¶è¡Œåè°ƒå™¨

æ ¸å¿ƒè®¾è®¡ï¼šä¸‰å¤§äº‘å¹³å°åŒæ—¶è°ƒç”¨ï¼Œäº¤å‰éªŒè¯ä¿¡æ¯å‡†ç¡®æ€§
ç±»ä¼¼DeepSeekçš„åŒæ¨¡å‹æŠ•ç¥¨æ€æƒ³ï¼Œä½†åº”ç”¨äºæƒ…æŠ¥æ”¶é›†
"""

from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging
import asyncio
from collections import Counter

from app.core.config import settings
from .platforms.cloud_adapters import (
    BaiduQwenAdapter,
    TencentQwenAdapter,
    VolcanoQwenAdapter,
    AWSQwenAdapter
)

logger = logging.getLogger(__name__)


class CloudPlatformCoordinator:
    """
    äº‘å¹³å°å¹¶è¡Œåè°ƒå™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. åŒæ—¶è°ƒç”¨ä¸‰å¤§äº‘å¹³å°ï¼ˆç™¾åº¦+è…¾è®¯+ç«å±±ï¼‰
    2. æ±‡æ€»å’Œå¯¹æ¯”ä¸‰ä¸ªå¹³å°çš„æœç´¢ç»“æœ
    3. äº¤å‰éªŒè¯ä¿¡æ¯å‡†ç¡®æ€§
    4. è®¡ç®—ç½®ä¿¡åº¦è¯„åˆ†ï¼ˆåŸºäºå¹³å°å…±è¯†åº¦ï¼‰
    
    å·¥ä½œæµç¨‹ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   æ”¶åˆ°æƒ…æŠ¥éœ€æ±‚   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  åŒæ—¶è°ƒç”¨ä¸‰å¹³å°  â”‚
    â”‚ â”œâ”€ ç™¾åº¦æ™ºèƒ½äº‘   â”‚
    â”‚ â”œâ”€ è…¾è®¯äº‘       â”‚
    â”‚ â””â”€ ç«å±±å¼•æ“     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  æ±‡æ€»ä¸‰ä»½ç»“æœ    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ äº¤å‰éªŒè¯ä¸èåˆ   â”‚
    â”‚ â”œâ”€ å…±åŒä¿¡æ¯     â”‚
    â”‚ â”œâ”€ éƒ¨åˆ†ä¿¡æ¯     â”‚
    â”‚ â””â”€ å•æºä¿¡æ¯     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ è¾“å‡ºç»¼åˆæƒ…æŠ¥æŠ¥å‘Š â”‚
    â”‚ (å«ç½®ä¿¡åº¦è¯„åˆ†)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    def __init__(self):
        """åˆå§‹åŒ–äº‘å¹³å°åè°ƒå™¨"""
        self.platforms: Dict[str, Any] = {}
        self._initialize_platforms()
        logger.info(f"âœ… äº‘å¹³å°åè°ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œå·²åŠ è½½ {len(self.platforms)} ä¸ªå¹³å°")
    
    def _initialize_platforms(self):
        """åˆå§‹åŒ–äº‘å¹³å°é€‚é…å™¨"""
        # ç™¾åº¦æ™ºèƒ½äº‘
        if settings.ENABLE_BAIDU_QWEN and settings.BAIDU_QWEN_API_KEY:
            self.platforms["baidu"] = BaiduQwenAdapter(
                api_key=settings.BAIDU_QWEN_API_KEY,
                base_url=settings.BAIDU_QWEN_BASE_URL,
                enabled=settings.ENABLE_BAIDU_QWEN
            )
            logger.info("âœ“ ç™¾åº¦æ™ºèƒ½äº‘å¹³å°å·²åŠ è½½")
        
        # è…¾è®¯äº‘
        if settings.ENABLE_TENCENT_QWEN and settings.TENCENT_QWEN_API_KEY:
            self.platforms["tencent"] = TencentQwenAdapter(
                api_key=settings.TENCENT_QWEN_API_KEY,
                base_url=settings.TENCENT_QWEN_BASE_URL,
                enabled=settings.ENABLE_TENCENT_QWEN
            )
            logger.info("âœ“ è…¾è®¯äº‘å¹³å°å·²åŠ è½½")
        
        # ç«å±±å¼•æ“
        if settings.ENABLE_VOLCANO_QWEN and settings.VOLCANO_QWEN_API_KEY:
            self.platforms["volcano"] = VolcanoQwenAdapter(
                api_key=settings.VOLCANO_QWEN_API_KEY,
                base_url=settings.VOLCANO_QWEN_BASE_URL,
                enabled=settings.ENABLE_VOLCANO_QWEN
            )
            logger.info("âœ“ ç«å±±å¼•æ“å¹³å°å·²åŠ è½½")
        
        # AWSï¼ˆå¯é€‰ï¼‰
        if settings.ENABLE_AWS_QWEN and settings.AWS_QWEN_API_KEY:
            self.platforms["aws"] = AWSQwenAdapter(
                api_key=settings.AWS_QWEN_API_KEY,
                base_url=settings.AWS_QWEN_BASE_URL,
                enabled=settings.ENABLE_AWS_QWEN
            )
            logger.info("âœ“ AWSå¹³å°å·²åŠ è½½")
    
    async def parallel_search_and_verify(
        self,
        data_sources: Dict[str, Any],
        query_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        å¹¶è¡Œæœç´¢ä¸äº¤å‰éªŒè¯ï¼ˆæ ¸å¿ƒæ–¹æ³•ï¼‰
        
        Args:
            data_sources: åŸå§‹æ•°æ®æº
            query_context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
        
        Returns:
            ç»¼åˆæƒ…æŠ¥æŠ¥å‘Šï¼ˆå«ç½®ä¿¡åº¦è¯„åˆ†ï¼‰
        """
        start_time = datetime.now()
        
        if len(self.platforms) < 2:
            logger.warning("âš ï¸  å¯ç”¨å¹³å°å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡Œäº¤å‰éªŒè¯ï¼")
            return self._fallback_response("å¯ç”¨å¹³å°ä¸è¶³")
        
        logger.info(f"ğŸ¯ å¼€å§‹å¹¶è¡Œè°ƒç”¨ {len(self.platforms)} ä¸ªäº‘å¹³å°è¿›è¡Œäº¤å‰éªŒè¯...")
        
        # === æ­¥éª¤1: åŒæ—¶è°ƒç”¨æ‰€æœ‰å¹³å° ===
        platform_results = await self._call_platforms_parallel(data_sources, query_context)
        
        if not platform_results:
            logger.error("âŒ æ‰€æœ‰å¹³å°è°ƒç”¨å¤±è´¥ï¼")
            return self._fallback_response("æ‰€æœ‰å¹³å°å¤±è´¥")
        
        logger.info(f"âœ“ æˆåŠŸè·å– {len(platform_results)} ä¸ªå¹³å°çš„ç»“æœ")
        
        # === æ­¥éª¤2: äº¤å‰éªŒè¯ä¸ä¿¡æ¯èåˆ ===
        verified_intelligence = self._cross_verify_results(platform_results)
        
        # === æ­¥éª¤3: è®¡ç®—ç»¼åˆç½®ä¿¡åº¦ ===
        confidence_score = self._calculate_confidence(platform_results, verified_intelligence)
        
        # === æ­¥éª¤4: æ„å»ºæœ€ç»ˆæŠ¥å‘Š ===
        final_report = {
            "intelligence_summary": verified_intelligence["summary"],
            "key_findings": verified_intelligence["high_confidence_findings"],
            "risk_warnings": verified_intelligence["risk_warnings"],
            "confidence": confidence_score,
            "verification_metadata": {
                "total_platforms_called": len(self.platforms),
                "successful_platforms": len(platform_results),
                "platform_consensus": verified_intelligence["consensus_rate"],
                "high_confidence_items": len(verified_intelligence["high_confidence_findings"]),
                "medium_confidence_items": len(verified_intelligence["medium_confidence_findings"]),
                "low_confidence_items": len(verified_intelligence["low_confidence_findings"]),
                "processing_time_seconds": (datetime.now() - start_time).total_seconds(),
                "timestamp": datetime.now().isoformat()
            },
            "platform_details": [
                {
                    "platform": name,
                    "success": True,
                    "key_findings_count": len(result.get("key_findings", [])),
                    "confidence": result.get("confidence", 0)
                }
                for name, result in platform_results.items()
            ]
        }
        
        logger.info(f"âœ… å¹¶è¡ŒéªŒè¯å®Œæˆ: ç½®ä¿¡åº¦={confidence_score:.2f}, å¹³å°å…±è¯†åº¦={verified_intelligence['consensus_rate']:.1%}")
        
        return final_report
    
    async def _call_platforms_parallel(
        self,
        data_sources: Dict[str, Any],
        query_context: Optional[Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        å¹¶è¡Œè°ƒç”¨æ‰€æœ‰å¹³å°
        
        Returns:
            {platform_name: result_dict}
        """
        tasks = []
        platform_names = []
        
        for name, platform in self.platforms.items():
            if platform.enabled:
                tasks.append(platform.analyze(data_sources, query_context))
                platform_names.append(name)
        
        if not tasks:
            return {}
        
        logger.info(f"ğŸ“¡ åŒæ—¶è°ƒç”¨ {len(tasks)} ä¸ªå¹³å°: {', '.join(platform_names)}")
        
        # ä½¿ç”¨asyncio.gatherå¹¶è¡Œæ‰§è¡Œï¼Œreturn_exceptions=Trueé¿å…ä¸€ä¸ªå¤±è´¥å½±å“å…¶ä»–
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ”¶é›†æˆåŠŸçš„ç»“æœ
        successful_results = {}
        for name, result in zip(platform_names, results):
            if isinstance(result, Exception):
                logger.warning(f"âš ï¸  {name} å¹³å°è°ƒç”¨å¤±è´¥: {result}")
            elif isinstance(result, dict) and not result.get("error"):
                successful_results[name] = result
                logger.info(f"âœ“ {name} å¹³å°è¿”å›æˆåŠŸ")
            else:
                logger.warning(f"âš ï¸  {name} å¹³å°è¿”å›é”™è¯¯: {result.get('error', 'Unknown')}")
        
        return successful_results
    
    def _cross_verify_results(
        self,
        platform_results: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        äº¤å‰éªŒè¯ç»“æœ
        
        æ ¸å¿ƒé€»è¾‘ï¼š
        1. æå–æ‰€æœ‰å¹³å°çš„key_findings
        2. å¯¹æ¯”ç›¸ä¼¼åº¦ï¼Œè¯†åˆ«å…±åŒä¿¡æ¯
        3. åˆ†ç±»ï¼šé«˜ç½®ä¿¡åº¦ï¼ˆå¤šå¹³å°å…±è¯†ï¼‰ã€ä¸­ç½®ä¿¡åº¦ï¼ˆéƒ¨åˆ†å…±è¯†ï¼‰ã€ä½ç½®ä¿¡åº¦ï¼ˆå•æºç‹¬æœ‰ï¼‰
        
        Returns:
            éªŒè¯åçš„æƒ…æŠ¥ï¼ˆåˆ†ç½®ä¿¡åº¦ç­‰çº§ï¼‰
        """
        all_findings = []
        platform_analyses = []
        
        # æ”¶é›†æ‰€æœ‰å¹³å°çš„å‘ç°å’Œåˆ†æ
        for platform_name, result in platform_results.items():
            findings = result.get("key_findings", [])
            analysis = result.get("analysis", "")
            
            for finding in findings:
                all_findings.append({
                    "content": finding,
                    "source": platform_name,
                    "original_confidence": result.get("confidence", 0.5)
                })
            
            if analysis:
                platform_analyses.append({
                    "platform": platform_name,
                    "analysis": analysis
                })
        
        # ç®€å•çš„ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¯ä»¥åç»­ä¼˜åŒ–ä¸ºå‘é‡ç›¸ä¼¼åº¦ï¼‰
        finding_groups = self._group_similar_findings(all_findings)
        
        # åˆ†ç±»
        high_confidence_findings = []  # 3ä¸ªæˆ–æ›´å¤šå¹³å°å…±è¯†
        medium_confidence_findings = []  # 2ä¸ªå¹³å°å…±è¯†
        low_confidence_findings = []  # å•å¹³å°ç‹¬æœ‰
        
        total_platforms = len(platform_results)
        
        for group in finding_groups:
            source_count = len(set([f["source"] for f in group]))
            consensus_rate = source_count / total_platforms
            
            # åˆå¹¶å†…å®¹
            merged_content = self._merge_findings(group)
            
            if source_count >= 3 or (source_count == 2 and total_platforms == 2):
                # é«˜ç½®ä¿¡åº¦ï¼šå¤šå¹³å°å…±è¯†
                high_confidence_findings.append({
                    "content": merged_content,
                    "consensus_platforms": source_count,
                    "total_platforms": total_platforms,
                    "sources": list(set([f["source"] for f in group]))
                })
            elif source_count == 2:
                # ä¸­ç½®ä¿¡åº¦ï¼šéƒ¨åˆ†å…±è¯†
                medium_confidence_findings.append({
                    "content": merged_content,
                    "consensus_platforms": source_count,
                    "sources": list(set([f["source"] for f in group]))
                })
            else:
                # ä½ç½®ä¿¡åº¦ï¼šå•æºç‹¬æœ‰
                low_confidence_findings.append({
                    "content": merged_content,
                    "source": group[0]["source"]
                })
        
        # ç”Ÿæˆç»¼åˆæ‘˜è¦
        summary = self._generate_summary(
            high_confidence_findings,
            medium_confidence_findings,
            platform_analyses
        )
        
        # æå–é£é™©è­¦å‘Šï¼ˆé«˜ç½®ä¿¡åº¦ä¸­çš„è´Ÿé¢ä¿¡æ¯ï¼‰
        risk_warnings = self._extract_risk_warnings(high_confidence_findings, medium_confidence_findings)
        
        # è®¡ç®—å…±è¯†åº¦
        total_findings = len(all_findings)
        high_conf_count = sum([f["consensus_platforms"] for f in high_confidence_findings])
        consensus_rate = high_conf_count / total_findings if total_findings > 0 else 0
        
        return {
            "summary": summary,
            "high_confidence_findings": high_confidence_findings,
            "medium_confidence_findings": medium_confidence_findings,
            "low_confidence_findings": low_confidence_findings,
            "risk_warnings": risk_warnings,
            "consensus_rate": consensus_rate
        }
    
    def _group_similar_findings(
        self,
        findings: List[Dict[str, Any]]
    ) -> List[List[Dict[str, Any]]]:
        """
        å°†ç›¸ä¼¼çš„å‘ç°åˆ†ç»„
        
        ç®€åŒ–ç‰ˆï¼šåŸºäºå…³é”®è¯åŒ¹é…
        TODO: åç»­å¯ä»¥å‡çº§ä¸ºå‘é‡ç›¸ä¼¼åº¦åŒ¹é…
        """
        groups = []
        used_indices = set()
        
        for i, finding1 in enumerate(findings):
            if i in used_indices:
                continue
            
            group = [finding1]
            used_indices.add(i)
            
            content1 = finding1["content"].lower()
            
            for j, finding2 in enumerate(findings):
                if j <= i or j in used_indices:
                    continue
                
                content2 = finding2["content"].lower()
                
                # ç®€å•çš„å…³é”®è¯é‡å æ£€æµ‹
                words1 = set(content1.split())
                words2 = set(content2.split())
                
                overlap = len(words1 & words2) / max(len(words1), len(words2))
                
                if overlap > 0.3:  # 30%çš„è¯é‡å è®¤ä¸ºç›¸ä¼¼
                    group.append(finding2)
                    used_indices.add(j)
            
            groups.append(group)
        
        return groups
    
    def _merge_findings(self, findings: List[Dict[str, Any]]) -> str:
        """åˆå¹¶ç›¸ä¼¼å‘ç°çš„å†…å®¹"""
        if len(findings) == 1:
            return findings[0]["content"]
        
        # é€‰æ‹©æœ€è¯¦ç»†çš„é‚£ä¸ª
        longest = max(findings, key=lambda f: len(f["content"]))
        return longest["content"]
    
    def _generate_summary(
        self,
        high_confidence: List[Dict],
        medium_confidence: List[Dict],
        platform_analyses: List[Dict]
    ) -> str:
        """ç”Ÿæˆç»¼åˆæ‘˜è¦"""
        summary_parts = []
        
        if high_confidence:
            summary_parts.append(f"ã€é«˜ç½®ä¿¡åº¦æƒ…æŠ¥ã€‘({len(high_confidence)}æ¡ï¼Œå¤šå¹³å°å…±è¯†):")
            for i, item in enumerate(high_confidence[:5], 1):  # æœ€å¤šæ˜¾ç¤º5æ¡
                summary_parts.append(f"{i}. {item['content']}")
        
        if medium_confidence:
            summary_parts.append(f"\nã€ä¸­ç½®ä¿¡åº¦æƒ…æŠ¥ã€‘({len(medium_confidence)}æ¡ï¼Œéƒ¨åˆ†å¹³å°å…±è¯†):")
            for i, item in enumerate(medium_confidence[:3], 1):  # æœ€å¤šæ˜¾ç¤º3æ¡
                summary_parts.append(f"{i}. {item['content']}")
        
        return "\n".join(summary_parts)
    
    def _extract_risk_warnings(
        self,
        high_confidence: List[Dict],
        medium_confidence: List[Dict]
    ) -> List[str]:
        """æå–é£é™©è­¦å‘Šï¼ˆè¯†åˆ«è´Ÿé¢å…³é”®è¯ï¼‰"""
        risk_keywords = ["ä¸‹è·Œ", "æš´è·Œ", "é£é™©", "è­¦å‘Š", "ç›‘ç®¡", "ç¦æ­¢", "å´©ç›˜", "æŠ›å”®", 
                        "bearish", "dump", "crash", "ban", "regulation", "risk"]
        
        warnings = []
        
        for item in high_confidence + medium_confidence:
            content = item["content"].lower()
            if any(keyword in content for keyword in risk_keywords):
                warnings.append(item["content"])
        
        return warnings[:5]  # æœ€å¤šè¿”å›5æ¡
    
    def _calculate_confidence(
        self,
        platform_results: Dict[str, Dict[str, Any]],
        verified_intelligence: Dict[str, Any]
    ) -> float:
        """
        è®¡ç®—ç»¼åˆç½®ä¿¡åº¦
        
        è€ƒè™‘å› ç´ ï¼š
        1. å¹³å°æ•°é‡ï¼ˆæ›´å¤šå¹³å° = æ›´é«˜ç½®ä¿¡åº¦ï¼‰
        2. å…±è¯†ç‡ï¼ˆæ›´å¤šå…±è¯† = æ›´é«˜ç½®ä¿¡åº¦ï¼‰
        3. å•å¹³å°çš„ç½®ä¿¡åº¦
        """
        if not platform_results:
            return 0.0
        
        # å› ç´ 1: å¹³å°æ•°é‡åŠ æˆ
        platform_count = len(platform_results)
        platform_factor = min(1.0, platform_count / 3)  # 3ä¸ªå¹³å°æ»¡åˆ†
        
        # å› ç´ 2: å…±è¯†ç‡
        consensus_rate = verified_intelligence["consensus_rate"]
        
        # å› ç´ 3: å¹³å‡å•å¹³å°ç½®ä¿¡åº¦
        avg_platform_confidence = sum([
            r.get("confidence", 0.5) for r in platform_results.values()
        ]) / len(platform_results)
        
        # ç»¼åˆè®¡ç®—
        final_confidence = (
            platform_factor * 0.3 +
            consensus_rate * 0.4 +
            avg_platform_confidence * 0.3
        )
        
        return min(1.0, max(0.0, final_confidence))
    
    def _fallback_response(self, reason: str) -> Dict[str, Any]:
        """é™çº§å“åº”"""
        return {
            "intelligence_summary": f"æƒ…æŠ¥æ”¶é›†å¤±è´¥: {reason}",
            "key_findings": [],
            "risk_warnings": [],
            "confidence": 0.0,
            "verification_metadata": {
                "total_platforms_called": len(self.platforms),
                "successful_platforms": 0,
                "error": reason,
                "timestamp": datetime.now().isoformat()
            },
            "platform_details": []
        }


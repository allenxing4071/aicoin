"""Mid-Term Intelligence Analyzer - Qwenæƒ…æŠ¥å‘˜ä¸­æœŸåˆ†æå±‚"""

from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from collections import defaultdict
import logging
import statistics

logger = logging.getLogger(__name__)


class MidTermIntelligenceAnalyzer:
    """
    Qwenæƒ…æŠ¥å‘˜ä¸­æœŸåˆ†æå±‚ï¼ˆLayer 2ï¼‰
    
    èŒè´£ï¼š
    1. åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
    2. è®¡ç®—ä¿¡æ¯æºæƒé‡
    3. è¯†åˆ«é«˜ä»·å€¼ä¿¡æ¯ç‰¹å¾
    4. å‘é‡åŒ–å‡†å¤‡
    
    å·¥ä½œå‘¨æœŸï¼šæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
    æ•°æ®æ¥æºï¼šä»çŸ­æœŸç¼“å­˜ä¸­è¯»å–
    è¾“å‡ºï¼šæƒé‡æ›´æ–°å»ºè®® + å‘é‡åŒ–å€™é€‰
    """
    
    def __init__(
        self,
        redis_client,
        db_session
    ):
        """
        åˆå§‹åŒ–ä¸­æœŸåˆ†æå™¨
        
        Args:
            redis_client: Rediså®¢æˆ·ç«¯
            db_session: æ•°æ®åº“ä¼šè¯
        """
        self.redis = redis_client
        self.db = db_session
        self.namespace = "qwen:intelligence:mid_term"
        
        logger.info("âœ… Qwenæƒ…æŠ¥å‘˜ä¸­æœŸåˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def analyze_user_behavior(
        self,
        time_window_hours: int = 24
    ) -> Dict[str, Any]:
        """
        åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
        
        Args:
            time_window_hours: åˆ†ææ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
        
        Returns:
            è¡Œä¸ºåˆ†æç»“æœ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹åˆ†æç”¨æˆ·è¡Œä¸ºï¼ˆ{time_window_hours}å°æ—¶ï¼‰...")
            
            # è·å–äº¤äº’ç»Ÿè®¡
            interaction_stats_key = "qwen:intelligence:stats:interactions"
            raw_stats = await self.redis.hgetall(interaction_stats_key)
            
            interaction_counts = {
                k: int(v) for k, v in raw_stats.items()
            } if raw_stats else {}
            
            # è®¡ç®—æ€»äº¤äº’æ•°
            total_interactions = sum(interaction_counts.values())
            
            # è®¡ç®—å„ç±»äº¤äº’å æ¯”
            interaction_distribution = {}
            if total_interactions > 0:
                for interaction_type, count in interaction_counts.items():
                    interaction_distribution[interaction_type] = {
                        "count": count,
                        "percentage": (count / total_interactions) * 100
                    }
            
            # è¯†åˆ«é«˜ä»·å€¼è¡Œä¸º
            high_value_actions = ["bookmark", "share", "deep_read"]
            high_value_count = sum(
                interaction_counts.get(action, 0)
                for action in high_value_actions
            )
            
            engagement_rate = (
                (high_value_count / total_interactions * 100)
                if total_interactions > 0
                else 0.0
            )
            
            result = {
                "time_window_hours": time_window_hours,
                "total_interactions": total_interactions,
                "interaction_distribution": interaction_distribution,
                "high_value_actions_count": high_value_count,
                "engagement_rate": engagement_rate,
                "analyzed_at": datetime.now().isoformat()
            }
            
            # ç¼“å­˜åˆ†æç»“æœ
            await self._cache_analysis_result("user_behavior", result)
            
            logger.info(
                f"âœ… ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ: "
                f"æ€»äº¤äº’{total_interactions}æ¬¡, "
                f"å‚ä¸åº¦{engagement_rate:.1f}%"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ç”¨æˆ·è¡Œä¸ºåˆ†æå¤±è´¥: {e}", exc_info=True)
            return {}
    
    async def calculate_source_weights(
        self,
        report_ids: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """
        è®¡ç®—ä¿¡æ¯æºæƒé‡
        
        åŸºäºä»¥ä¸‹å› ç´ ï¼š
        1. ä½¿ç”¨é¢‘ç‡
        2. ç”¨æˆ·åé¦ˆï¼ˆç‚¹å‡»ã€åœç•™æ—¶é—´ï¼‰
        3. å†³ç­–å½±å“ï¼ˆæ˜¯å¦è¢«é‡‡çº³ï¼‰
        4. ä¿¡æ¯å‡†ç¡®æ€§
        
        Args:
            report_ids: è¦åˆ†æçš„æŠ¥å‘ŠIDåˆ—è¡¨ï¼ˆNoneåˆ™åˆ†ææ‰€æœ‰æœ€è¿‘æŠ¥å‘Šï¼‰
        
        Returns:
            {source_name: weight} å­—å…¸
        """
        try:
            logger.info("ğŸ“Š å¼€å§‹è®¡ç®—ä¿¡æ¯æºæƒé‡...")
            
            source_metrics = defaultdict(lambda: {
                "usage_count": 0,
                "positive_interactions": 0,
                "total_interactions": 0,
                "decision_influenced": 0,
                "accuracy_score": 0.5
            })
            
            # å¦‚æœæœªæŒ‡å®šreport_idsï¼Œè·å–æœ€è¿‘çš„æŠ¥å‘Š
            if report_ids is None:
                report_ids = await self._get_recent_report_ids(limit=100)
            
            # åˆ†ææ¯ä¸ªæŠ¥å‘Š
            for report_id in report_ids:
                report_data = await self._get_cached_report(report_id)
                if not report_data:
                    continue
                
                # æå–æ•°æ®æºä¿¡æ¯
                sources = self._extract_sources(report_data)
                
                # è·å–äº¤äº’æ•°æ®
                interactions = await self._get_interactions(report_id)
                
                # æ›´æ–°æ¯ä¸ªæºçš„æŒ‡æ ‡
                for source_name in sources:
                    metrics = source_metrics[source_name]
                    metrics["usage_count"] += 1
                    
                    # ç»Ÿè®¡äº¤äº’
                    positive_actions = ["click", "bookmark", "share"]
                    for interaction in interactions:
                        metrics["total_interactions"] += 1
                        if interaction.get("type") in positive_actions:
                            metrics["positive_interactions"] += 1
                    
                    # æ£€æŸ¥æ˜¯å¦å½±å“å†³ç­–
                    if report_data.get("influenced_decision", False):
                        metrics["decision_influenced"] += 1
            
            # è®¡ç®—æƒé‡
            weights = {}
            for source_name, metrics in source_metrics.items():
                weight = self._compute_weight(metrics)
                weights[source_name] = weight
            
            # å½’ä¸€åŒ–æƒé‡ï¼ˆç¡®ä¿æ€»å’Œä¸º1.0ï¼‰
            if weights:
                total_weight = sum(weights.values())
                if total_weight > 0:
                    weights = {
                        k: v / total_weight
                        for k, v in weights.items()
                    }
            
            # ç¼“å­˜æƒé‡ç»“æœ
            await self._cache_analysis_result("source_weights", weights)
            
            logger.info(f"âœ… ä¿¡æ¯æºæƒé‡è®¡ç®—å®Œæˆ: {len(weights)} ä¸ªæº")
            
            return weights
            
        except Exception as e:
            logger.error(f"âŒ è®¡ç®—ä¿¡æ¯æºæƒé‡å¤±è´¥: {e}", exc_info=True)
            return {}
    
    def _compute_weight(self, metrics: Dict[str, Any]) -> float:
        """
        è®¡ç®—å•ä¸ªæºçš„æƒé‡
        
        æƒé‡å…¬å¼ï¼š
        weight = (
            0.3 * usage_frequency +
            0.3 * engagement_rate +
            0.2 * decision_influence +
            0.2 * accuracy_score
        )
        """
        usage_count = metrics["usage_count"]
        total_interactions = metrics["total_interactions"]
        positive_interactions = metrics["positive_interactions"]
        decision_influenced = metrics["decision_influenced"]
        
        # å½’ä¸€åŒ–ä½¿ç”¨é¢‘ç‡ï¼ˆå‡è®¾æœ€å¤§100æ¬¡ï¼‰
        usage_frequency = min(usage_count / 100.0, 1.0)
        
        # å‚ä¸ç‡
        engagement_rate = (
            positive_interactions / total_interactions
            if total_interactions > 0
            else 0.5
        )
        
        # å†³ç­–å½±å“ç‡
        decision_influence = (
            decision_influenced / usage_count
            if usage_count > 0
            else 0.0
        )
        
        # å‡†ç¡®æ€§è¯„åˆ†ï¼ˆç›®å‰ä½¿ç”¨é»˜è®¤å€¼ï¼‰
        accuracy_score = metrics.get("accuracy_score", 0.5)
        
        # åŠ æƒè®¡ç®—
        weight = (
            0.3 * usage_frequency +
            0.3 * engagement_rate +
            0.2 * decision_influence +
            0.2 * accuracy_score
        )
        
        return weight
    
    async def identify_high_value_patterns(self) -> List[Dict[str, Any]]:
        """
        è¯†åˆ«é«˜ä»·å€¼ä¿¡æ¯æ¨¡å¼
        
        Returns:
            é«˜ä»·å€¼æ¨¡å¼åˆ—è¡¨
        """
        try:
            logger.info("ğŸ” è¯†åˆ«é«˜ä»·å€¼ä¿¡æ¯æ¨¡å¼...")
            
            patterns = []
            
            # æ¨¡å¼1ï¼šé«˜å‚ä¸åº¦çš„ä¸»é¢˜
            # æ¨¡å¼2ï¼šé«˜å‡†ç¡®æ€§çš„ä¿¡æ¯ç±»å‹
            # æ¨¡å¼3ï¼šé«˜å½±å“åŠ›çš„äº‹ä»¶ç±»åˆ«
            
            # TODO: å®ç°æ¨¡å¼è¯†åˆ«ç®—æ³•
            # è¿™é‡Œæä¾›åŸºç¡€æ¡†æ¶
            
            patterns.append({
                "pattern_type": "high_engagement_topic",
                "description": "é«˜å‚ä¸åº¦ä¸»é¢˜",
                "examples": ["ç›‘ç®¡æ”¿ç­–", "æŠ€æœ¯å‡çº§", "æœºæ„åŠ¨å‘"],
                "confidence": 0.75
            })
            
            await self._cache_analysis_result("high_value_patterns", patterns)
            
            logger.info(f"âœ… è¯†åˆ«åˆ° {len(patterns)} ä¸ªé«˜ä»·å€¼æ¨¡å¼")
            
            return patterns
            
        except Exception as e:
            logger.error(f"âŒ è¯†åˆ«é«˜ä»·å€¼æ¨¡å¼å¤±è´¥: {e}")
            return []
    
    async def prepare_vectorization_candidates(
        self,
        min_interaction_threshold: int = 3
    ) -> List[Dict[str, Any]]:
        """
        å‡†å¤‡å¾…å‘é‡åŒ–çš„å€™é€‰æ•°æ®
        
        é€‰æ‹©æ ‡å‡†ï¼š
        1. äº¤äº’æ¬¡æ•° >= é˜ˆå€¼
        2. æœ‰æ­£é¢åé¦ˆ
        3. æ—¶é—´åœ¨24å°æ—¶å†…
        
        Args:
            min_interaction_threshold: æœ€å°äº¤äº’æ¬¡æ•°
        
        Returns:
            å€™é€‰åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ“¦ å‡†å¤‡å‘é‡åŒ–å€™é€‰ï¼ˆé˜ˆå€¼={min_interaction_threshold}ï¼‰...")
            
            candidates = []
            
            # è·å–æœ€è¿‘çš„æŠ¥å‘Š
            report_ids = await self._get_recent_report_ids(limit=50)
            
            for report_id in report_ids:
                # è·å–äº¤äº’æ•°æ®
                interactions = await self._get_interactions(report_id)
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶
                if len(interactions) >= min_interaction_threshold:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ­£é¢åé¦ˆ
                    has_positive = any(
                        i.get("type") in ["bookmark", "share"]
                        for i in interactions
                    )
                    
                    if has_positive:
                        report_data = await self._get_cached_report(report_id)
                        if report_data:
                            candidates.append({
                                "report_id": report_id,
                                "report_data": report_data,
                                "interaction_count": len(interactions),
                                "should_vectorize": True,
                                "priority": self._calculate_priority(interactions)
                            })
            
            # æŒ‰ä¼˜å…ˆçº§æ’åº
            candidates.sort(key=lambda x: x["priority"], reverse=True)
            
            logger.info(f"âœ… å‡†å¤‡äº† {len(candidates)} ä¸ªå‘é‡åŒ–å€™é€‰")
            
            return candidates
            
        except Exception as e:
            logger.error(f"âŒ å‡†å¤‡å‘é‡åŒ–å€™é€‰å¤±è´¥: {e}")
            return []
    
    def _calculate_priority(self, interactions: List[Dict[str, Any]]) -> float:
        """è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°"""
        score = 0.0
        
        for interaction in interactions:
            interaction_type = interaction.get("type", "")
            
            # ä¸åŒäº¤äº’ç±»å‹çš„æƒé‡
            weights = {
                "view": 0.1,
                "click": 0.3,
                "bookmark": 1.0,
                "share": 1.5,
                "deep_read": 0.8
            }
            
            score += weights.get(interaction_type, 0.1)
        
        return score
    
    def _extract_sources(self, report_data: Dict[str, Any]) -> List[str]:
        """ä»æŠ¥å‘Šä¸­æå–æ•°æ®æº"""
        sources = []
        
        # ä»å¹³å°è´¡çŒ®ä¸­æå–
        platform_contributions = report_data.get("platform_contributions", {})
        for platform in platform_contributions.keys():
            sources.append(platform)
        
        # ä»åŸå§‹æ•°æ®ä¸­æå–
        # TODO: æ ¹æ®å®é™…æ•°æ®ç»“æ„æå–
        
        return sources
    
    async def _get_recent_report_ids(self, limit: int) -> List[str]:
        """è·å–æœ€è¿‘çš„æŠ¥å‘ŠID"""
        try:
            ids = await self.redis.zrevrange(
                "qwen:intelligence:reports:recent",
                0,
                limit - 1
            )
            return [id.decode() if isinstance(id, bytes) else id for id in ids]
        except:
            return []
    
    async def _get_cached_report(self, report_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„æŠ¥å‘Š"""
        try:
            import json
            key = f"qwen:intelligence:report:{report_id}"
            data = await self.redis.get(key)
            if data:
                return json.loads(data)
        except:
            pass
        return None
    
    async def _get_interactions(self, report_id: str) -> List[Dict[str, Any]]:
        """è·å–äº¤äº’è®°å½•"""
        try:
            import json
            key = f"qwen:intelligence:interactions:{report_id}"
            raw_data = await self.redis.lrange(key, 0, -1)
            return [json.loads(item) for item in raw_data]
        except:
            return []
    
    async def _cache_analysis_result(
        self,
        analysis_type: str,
        result: Any
    ) -> bool:
        """ç¼“å­˜åˆ†æç»“æœ"""
        try:
            import json
            key = f"{self.namespace}:analysis:{analysis_type}"
            serialized = json.dumps(result, default=str, ensure_ascii=False)
            await self.redis.setex(key, 3600 * 24, serialized)  # 24å°æ—¶
            return True
        except:
            return False
    
    async def get_analysis_summary(self) -> Dict[str, Any]:
        """è·å–åˆ†ææ‘˜è¦"""
        try:
            return {
                "user_behavior": await self._get_cached_analysis("user_behavior"),
                "source_weights": await self._get_cached_analysis("source_weights"),
                "high_value_patterns": await self._get_cached_analysis("high_value_patterns"),
                "last_analysis_time": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"âŒ è·å–åˆ†ææ‘˜è¦å¤±è´¥: {e}")
            return {}
    
    async def _get_cached_analysis(self, analysis_type: str) -> Optional[Any]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        try:
            import json
            key = f"{self.namespace}:analysis:{analysis_type}"
            data = await self.redis.get(key)
            if data:
                return json.loads(data)
        except:
            pass
        return None


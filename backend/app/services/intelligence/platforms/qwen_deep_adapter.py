"""Qwen Deep Analyzer Adapter - Qwenæ·±åº¦æ¨ç†é€‚é…å™¨ï¼ˆæ·±åº¦åˆ†æå¸ˆï¼‰"""

from typing import Dict, List, Any, Optional
from datetime import datetime
import logging
import openai
from .base_adapter import BasePlatformAdapter, PlatformRole

logger = logging.getLogger(__name__)


class QwenDeepAdapter(BasePlatformAdapter):
    """
    Qwenæ·±åº¦åˆ†æé€‚é…å™¨ - å¹³å°Cï¼šæ·±åº¦åˆ†æå¸ˆ
    
    èŒè´£ï¼š
    1. å¤æ‚æ¨ç†å’Œå…³è”åˆ†æ
    2. ç»¼åˆç ”åˆ¤å¤šæºä¿¡æ¯
    3. ç”Ÿæˆæ·±åº¦æ´å¯ŸæŠ¥å‘Š
    
    ç‰¹ç‚¹ï¼š
    - ä½¿ç”¨Qwençš„å¼ºå¤§æ¨ç†èƒ½åŠ›
    - æ·±åº¦åˆ†æï¼Œé«˜è´¨é‡è¾“å‡º
    - æˆæœ¬è¾ƒé«˜ï¼Œç”¨äºå…³é”®å†³ç­–
    """
    
    def __init__(
        self,
        api_key: str,
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        model: str = "qwen-plus",
        enabled: bool = True
    ):
        super().__init__(
            platform_name="Qwen Deep Analysis (æ·±åº¦åˆ†æ)",
            role=PlatformRole.DEEP_ANALYST,
            api_key=api_key,
            base_url=base_url,
            enabled=enabled
        )
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯ï¼ˆQwenå…¼å®¹ï¼‰
        self.client = openai.AsyncOpenAI(
            api_key=api_key,
            base_url=base_url
        )
        self.model = model
    
    async def analyze(
        self,
        data_sources: Dict[str, Any],
        query_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        æ·±åº¦ç»¼åˆåˆ†æ
        
        Args:
            data_sources: {
                "raw_data": {...},           # åŸå§‹æ•°æ®æº
                "free_platform_result": {...},  # å…è´¹å¹³å°ç­›é€‰ç»“æœ
                "search_result": {...}         # å®æ—¶æœç´¢ç»“æœ
            }
            query_context: æŸ¥è¯¢ä¸Šä¸‹æ–‡
        
        Returns:
            æ·±åº¦åˆ†æç»“æœ
        """
        try:
            logger.info("ğŸ§  Qwenæ·±åº¦åˆ†æå¹³å°å¼€å§‹ç»¼åˆç ”åˆ¤...")
            
            # æ„å»ºæ·±åº¦åˆ†æPrompt
            analysis_prompt = self._build_analysis_prompt(data_sources, query_context)
            
            # è°ƒç”¨Qwen API
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„åŠ å¯†è´§å¸å¸‚åœºæ·±åº¦åˆ†æå¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. ç»¼åˆåˆ†æå¤šæºä¿¡æ¯ï¼ˆæ–°é—»ã€é“¾ä¸Šæ•°æ®ã€å®æ—¶æœç´¢ç»“æœï¼‰
2. è¯†åˆ«æ·±å±‚å…³è”å’Œå› æœå…³ç³»
3. è¯„ä¼°æ½œåœ¨é£é™©å’Œæœºä¼š
4. æä¾›é«˜è´¨é‡çš„æŠ•èµ„æ´å¯Ÿ

ä½ çš„åˆ†æåº”è¯¥ï¼š
- åŸºäºäº‹å®å’Œæ•°æ®
- é€»è¾‘ä¸¥è°¨ã€æ¨ç†æ¸…æ™°
- è€ƒè™‘å¤šä¸ªç»´åº¦å’Œå¯èƒ½æ€§
- æä¾›å¯æ“ä½œçš„å»ºè®®"""
                    },
                    {
                        "role": "user",
                        "content": analysis_prompt
                    }
                ],
                temperature=0.5,  # å¹³è¡¡åˆ›é€ æ€§å’Œå‡†ç¡®æ€§
                max_tokens=2000,  # å…è®¸æ›´é•¿çš„æ·±åº¦åˆ†æ
            )
            
            analysis_text = response.choices[0].message.content
            
            # ä¼°ç®—æˆæœ¬
            usage = response.usage
            input_tokens = usage.prompt_tokens if usage else 0
            output_tokens = usage.completion_tokens if usage else 0
            cost = self._calculate_cost(input_tokens, output_tokens)
            
            # è§£æåˆ†æç»“æœ
            parsed_analysis = self._parse_analysis(analysis_text)
            
            # âœ… è®°å½•è°ƒç”¨ï¼ˆåŒ…å«tokenä¿¡æ¯ï¼‰
            await self._record_call(success=True, cost=cost, response_time=0.0, input_tokens=input_tokens, output_tokens=output_tokens)
            
            result = {
                "platform": self.platform_name,
                "role": self.role,
                "analysis": analysis_text,
                "confidence": parsed_analysis.get("confidence", 0.9),
                "key_findings": parsed_analysis.get("key_findings", []),
                "risk_factors": parsed_analysis.get("risk_factors", []),
                "opportunities": parsed_analysis.get("opportunities", []),
                "market_sentiment": parsed_analysis.get("sentiment", "neutral"),
                "sentiment_score": parsed_analysis.get("sentiment_score", 0.0),
                "timestamp": datetime.now(),
                "cost": cost,
                "tokens_used": {
                    "prompt": usage.prompt_tokens if usage else 0,
                    "completion": usage.completion_tokens if usage else 0,
                    "total": usage.total_tokens if usage else 0
                }
            }
            
            logger.info(f"âœ… Qwenæ·±åº¦åˆ†æå®Œæˆ: ç½®ä¿¡åº¦ {result['confidence']:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Qwenæ·±åº¦åˆ†æå¤±è´¥: {e}", exc_info=True)
            response_time = (datetime.now() - start_time).total_seconds() * 1000 if "start_time" in locals() else 0.0
            await self._record_call(success=False, cost=0.0, response_time=response_time)
            
            return {
                "platform": self.platform_name,
                "role": self.role,
                "analysis": "æ·±åº¦åˆ†ææš‚æ—¶ä¸å¯ç”¨",
                "confidence": 0.0,
                "key_findings": [],
                "risk_factors": [],
                "opportunities": [],
                "timestamp": datetime.now(),
                "cost": 0.0,
                "error": str(e)
            }
    
    def _build_analysis_prompt(
        self,
        data_sources: Dict[str, Any],
        query_context: Optional[Dict[str, Any]]
    ) -> str:
        """æ„å»ºæ·±åº¦åˆ†æPrompt"""
        prompt_parts = [
            "è¯·å¯¹ä»¥ä¸‹åŠ å¯†è´§å¸å¸‚åœºæƒ…æŠ¥è¿›è¡Œæ·±åº¦ç»¼åˆåˆ†æï¼š",
            "",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
            "ğŸ“Š åŸºç¡€ç­›é€‰ç»“æœï¼ˆå…è´¹å¹³å°ï¼‰",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        ]
        
        # æ·»åŠ å…è´¹å¹³å°ç»“æœ
        free_result = data_sources.get("free_platform_result", {})
        if free_result:
            prompt_parts.append(free_result.get("analysis", "æš‚æ— åŸºç¡€ç­›é€‰ç»“æœ"))
            findings = free_result.get("key_findings", [])
            if findings:
                prompt_parts.append("\nå…³é”®å‘ç°ï¼š")
                for i, finding in enumerate(findings, 1):
                    prompt_parts.append(f"{i}. {finding}")
        
        prompt_parts.extend([
            "",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
            "ğŸ” å®æ—¶æœç´¢ç»“æœï¼ˆDeepSeekæœç´¢ï¼‰",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
        ])
        
        # æ·»åŠ å®æ—¶æœç´¢ç»“æœ
        search_result = data_sources.get("search_result", {})
        if search_result:
            prompt_parts.append(search_result.get("analysis", "æš‚æ— å®æ—¶æœç´¢ç»“æœ"))
        
        prompt_parts.extend([
            "",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
            "ğŸ¯ æ·±åº¦åˆ†æä»»åŠ¡",
            "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
            "",
            "è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦ç»¼åˆåˆ†æï¼Œå¹¶æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š",
            "",
            "1. **ç»¼åˆåˆ†æ** (200-300å­—)ï¼š",
            "   - æ•´åˆå¤šæºä¿¡æ¯ï¼Œè¯†åˆ«å…³é”®è¶‹åŠ¿",
            "   - åˆ†æäº‹ä»¶ä¹‹é—´çš„å…³è”æ€§",
            "   - è¯„ä¼°å¸‚åœºæ•´ä½“çŠ¶å†µ",
            "",
            "2. **å¸‚åœºæƒ…ç»ª** (çœ‹æ¶¨/çœ‹è·Œ/ä¸­æ€§)ï¼š",
            "   - ç»™å‡ºæ˜ç¡®çš„æƒ…ç»ªåˆ¤æ–­",
            "   - æä¾›æƒ…ç»ªå¼ºåº¦åˆ†æ•° (-1.0 åˆ° 1.0)",
            "",
            "3. **é£é™©å› ç´ ** (åˆ—å‡º3-5ä¸ª)ï¼š",
            "   - è¯†åˆ«æ½œåœ¨çš„ä¸‹è¡Œé£é™©",
            "   - è¯„ä¼°é£é™©çš„ä¸¥é‡ç¨‹åº¦",
            "",
            "4. **æœºä¼šç‚¹** (åˆ—å‡º2-3ä¸ª)ï¼š",
            "   - è¯†åˆ«æ½œåœ¨çš„ä¸Šè¡Œæœºä¼š",
            "   - è¯„ä¼°æœºä¼šçš„å¯è¡Œæ€§",
            "",
            "5. **ç½®ä¿¡åº¦** (0.0-1.0)ï¼š",
            "   - å¯¹æœ¬æ¬¡åˆ†æçš„æ•´ä½“ç½®ä¿¡åº¦",
            "",
            "è¯·ä¿æŒå®¢è§‚ã€ä¸“ä¸šï¼ŒåŸºäºäº‹å®å’Œé€»è¾‘è¿›è¡Œæ¨ç†ã€‚"
        ])
        
        return "\n".join(prompt_parts)
    
    def _parse_analysis(self, analysis_text: str) -> Dict[str, Any]:
        """è§£æåˆ†æç»“æœ"""
        result = {
            "key_findings": [],
            "risk_factors": [],
            "opportunities": [],
            "sentiment": "neutral",
            "sentiment_score": 0.0,
            "confidence": 0.8
        }
        
        try:
            start_time = datetime.now()
            lines = analysis_text.split('\n')
            current_section = None
            
            for line in lines:
                line = line.strip()
                
                # è¯†åˆ«ç« èŠ‚
                if "é£é™©å› ç´ " in line or "Risk" in line:
                    current_section = "risks"
                elif "æœºä¼š" in line or "Opportunity" in line or "Opportunities" in line:
                    current_section = "opportunities"
                elif "å¸‚åœºæƒ…ç»ª" in line or "Sentiment" in line:
                    current_section = "sentiment"
                elif "ç½®ä¿¡åº¦" in line or "Confidence" in line:
                    current_section = "confidence"
                
                # æå–å†…å®¹
                if current_section == "risks" and line and (line[0].isdigit() or line.startswith('-')):
                    cleaned = line.lstrip('0123456789.â€¢-* \t')
                    if len(cleaned) > 5:
                        result["risk_factors"].append(cleaned)
                
                elif current_section == "opportunities" and line and (line[0].isdigit() or line.startswith('-')):
                    cleaned = line.lstrip('0123456789.â€¢-* \t')
                    if len(cleaned) > 5:
                        result["opportunities"].append(cleaned)
                
                elif current_section == "sentiment":
                    if "BULLISH" in line.upper() or "çœ‹æ¶¨" in line:
                        result["sentiment"] = "bullish"
                        result["sentiment_score"] = 0.6
                    elif "BEARISH" in line.upper() or "çœ‹è·Œ" in line:
                        result["sentiment"] = "bearish"
                        result["sentiment_score"] = -0.6
                    
                    # å°è¯•æå–åˆ†æ•°
                    import re
                    score_match = re.search(r'[-+]?\d*\.?\d+', line)
                    if score_match:
                        try:
                            score = float(score_match.group())
                            if -1.0 <= score <= 1.0:
                                result["sentiment_score"] = score
                        except:
                            pass
                
                elif current_section == "confidence":
                    import re
                    conf_match = re.search(r'\d*\.?\d+', line)
                    if conf_match:
                        try:
                            conf = float(conf_match.group())
                            if conf <= 1.0:
                                result["confidence"] = conf
                            elif conf <= 100:
                                result["confidence"] = conf / 100
                        except:
                            pass
            
            # é»˜è®¤å€¼
            if not result["risk_factors"]:
                result["risk_factors"] = ["å¸‚åœºæ³¢åŠ¨æ€§", "ç›‘ç®¡ä¸ç¡®å®šæ€§"]
            if not result["opportunities"]:
                result["opportunities"] = ["æŠ€æœ¯é¢çªç ´"]
            
            # æå–å…³é”®å‘ç°ï¼ˆä»å¼€å¤´éƒ¨åˆ†ï¼‰
            for line in lines[:20]:
                line = line.strip()
                if line and len(line) > 20 and (
                    line[0].isdigit() or 
                    line.startswith('â€¢') or 
                    line.startswith('-')
                ):
                    cleaned = line.lstrip('0123456789.â€¢-* \t')
                    if cleaned not in result["key_findings"]:
                        result["key_findings"].append(cleaned)
                        if len(result["key_findings"]) >= 5:
                            break
            
        except Exception as e:
            logger.error(f"è§£æåˆ†æç»“æœå¤±è´¥: {e}")
        
        return result
    
    def _calculate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """
        è®¡ç®—APIè°ƒç”¨æˆæœ¬ï¼ˆä½¿ç”¨ç»Ÿä¸€å®šä»·ç®¡ç†å™¨ï¼‰
        """
        from app.services.ai_pricing import get_pricing_manager
        
        pricing_manager = get_pricing_manager()
        
        # ä½¿ç”¨ç»Ÿä¸€çš„å®šä»·ç®¡ç†å™¨è®¡ç®—æˆæœ¬
        cost = pricing_manager.calculate_cost(
            provider=self.provider or "qwen",
            model="qwen-plus",  # é»˜è®¤ä½¿ç”¨ qwen-plus
            input_tokens=prompt_tokens,
            output_tokens=completion_tokens
        )
        
        return cost
    
    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        if not self.enabled:
            return False
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "ping"}],
                max_tokens=10,
                timeout=5
            )
            return bool(response)
        except Exception as e:
            logger.error(f"Qwenæ·±åº¦åˆ†æå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False


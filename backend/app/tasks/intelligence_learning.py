"""Intelligence Learning Tasks - Qwenæƒ…æŠ¥ç³»ç»ŸæŒç»­å­¦ä¹ å®šæ—¶ä»»åŠ¡"""

from celery import Celery
from celery.schedules import crontab
import logging
from datetime import datetime, timedelta

from app.core.redis_client import redis_client
from app.core.database import get_db
from app.services.intelligence.storage_layers import (
    MidTermIntelligenceAnalyzer,
    LongTermIntelligenceStore,
    IntelligenceVectorKB
)
from app.services.intelligence.source_weight_optimizer import SourceWeightOptimizer

logger = logging.getLogger(__name__)

# Celery appé…ç½®
celery_app = Celery("intelligence_learning")

# å®šæ—¶ä»»åŠ¡è°ƒåº¦é…ç½®
celery_app.conf.beat_schedule = {
    # æ¯å°æ—¶æ›´æ–°ä¿¡æ¯æºæƒé‡
    'optimize-source-weights-hourly': {
        'task': 'app.tasks.intelligence_learning.optimize_source_weights',
        'schedule': crontab(minute=0),  # æ¯å°æ—¶æ•´ç‚¹
    },
    
    # æ¯å°æ—¶åˆ†æç”¨æˆ·è¡Œä¸º
    'analyze-user-behavior-hourly': {
        'task': 'app.tasks.intelligence_learning.analyze_user_behavior',
        'schedule': crontab(minute=15),  # æ¯å°æ—¶15åˆ†
    },
    
    # æ¯æ—¥å‘é‡åŒ–ä»»åŠ¡
    'vectorize-intelligence-daily': {
        'task': 'app.tasks.intelligence_learning.vectorize_daily_intelligence',
        'schedule': crontab(hour=2, minute=0),  # æ¯å¤©å‡Œæ™¨2ç‚¹
    },
    
    # æ¯æ—¥æƒ…æŠ¥è´¨é‡è¯„ä¼°
    'evaluate-intelligence-quality-daily': {
        'task': 'app.tasks.intelligence_learning.evaluate_intelligence_quality',
        'schedule': crontab(hour=3, minute=0),  # æ¯å¤©å‡Œæ™¨3ç‚¹
    },
    
    # æ¯å‘¨æ¨¡å¼åˆ†æ
    'analyze-patterns-weekly': {
        'task': 'app.tasks.intelligence_learning.analyze_patterns_weekly',
        'schedule': crontab(day_of_week=1, hour=4, minute=0),  # æ¯å‘¨ä¸€å‡Œæ™¨4ç‚¹
    },
    
    # æ¯å‘¨ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    'generate-optimization-report-weekly': {
        'task': 'app.tasks.intelligence_learning.generate_optimization_report',
        'schedule': crontab(day_of_week=0, hour=23, minute=0),  # æ¯å‘¨æ—¥23ç‚¹
    },
    
    # æ¯4å°æ—¶è‡ªåŠ¨ç”Ÿæˆè¾©è®ºæŠ¥å‘Š
    'auto-generate-debate-report': {
        'task': 'app.tasks.intelligence_learning.auto_generate_debate_report',
        'schedule': crontab(minute=0, hour='*/4'),  # æ¯4å°æ—¶
    },
}


@celery_app.task(name='app.tasks.intelligence_learning.optimize_source_weights')
def optimize_source_weights():
    """
    æ¯å°æ—¶ä¼˜åŒ–ä¿¡æ¯æºæƒé‡
    
    èŒè´£ï¼š
    1. æ”¶é›†æœ€è¿‘1å°æ—¶çš„åé¦ˆæ•°æ®
    2. é‡æ–°è®¡ç®—å„æºæƒé‡
    3. æ›´æ–°åˆ°æ•°æ®åº“
    4. ç¼“å­˜åˆ°Redis
    """
    try:
        logger.info("ğŸ”§ å¼€å§‹æ‰§è¡Œä¿¡æ¯æºæƒé‡ä¼˜åŒ–ä»»åŠ¡...")
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        db = next(get_db())
        optimizer = SourceWeightOptimizer(
            redis_client=redis_client,
            db_session=db
        )
        
        # æ‰§è¡Œä¼˜åŒ–ï¼ˆ30å¤©çª—å£ï¼‰
        import asyncio
        optimized_weights = asyncio.run(
            optimizer.optimize_weights(time_window_days=30)
        )
        
        logger.info(
            f"âœ… ä¿¡æ¯æºæƒé‡ä¼˜åŒ–å®Œæˆ: "
            f"æ›´æ–°äº† {len(optimized_weights)} ä¸ªæºçš„æƒé‡"
        )
        
        return {
            "status": "success",
            "sources_optimized": len(optimized_weights),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ä¿¡æ¯æºæƒé‡ä¼˜åŒ–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@celery_app.task(name='app.tasks.intelligence_learning.analyze_user_behavior')
def analyze_user_behavior():
    """
    æ¯å°æ—¶åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼
    
    èŒè´£ï¼š
    1. ç»Ÿè®¡ç”¨æˆ·äº¤äº’è¡Œä¸º
    2. è¯†åˆ«é«˜ä»·å€¼è¡Œä¸ºæ¨¡å¼
    3. è®¡ç®—å‚ä¸åº¦æŒ‡æ ‡
    """
    try:
        logger.info("ğŸ“Š å¼€å§‹åˆ†æç”¨æˆ·è¡Œä¸ºæ¨¡å¼...")
        
        db = next(get_db())
        analyzer = MidTermIntelligenceAnalyzer(
            redis_client=redis_client,
            db_session=db
        )
        
        # æ‰§è¡Œåˆ†æï¼ˆ24å°æ—¶çª—å£ï¼‰
        import asyncio
        behavior_analysis = asyncio.run(
            analyzer.analyze_user_behavior(time_window_hours=24)
        )
        
        logger.info(
            f"âœ… ç”¨æˆ·è¡Œä¸ºåˆ†æå®Œæˆ: "
            f"æ€»äº¤äº’{behavior_analysis.get('total_interactions', 0)}æ¬¡, "
            f"å‚ä¸åº¦{behavior_analysis.get('engagement_rate', 0):.1f}%"
        )
        
        return {
            "status": "success",
            "analysis": behavior_analysis,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ç”¨æˆ·è¡Œä¸ºåˆ†æä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@celery_app.task(name='app.tasks.intelligence_learning.vectorize_daily_intelligence')
def vectorize_daily_intelligence():
    """
    æ¯æ—¥å‘é‡åŒ–ä»»åŠ¡
    
    èŒè´£ï¼š
    1. è·å–æ˜¨æ—¥çš„é«˜ä»·å€¼æƒ…æŠ¥
    2. å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°Qdrant
    3. å»ºç«‹çŸ¥è¯†ç§¯ç´¯
    """
    try:
        logger.info("ğŸ”® å¼€å§‹æ¯æ—¥æƒ…æŠ¥å‘é‡åŒ–ä»»åŠ¡...")
        
        db = next(get_db())
        analyzer = MidTermIntelligenceAnalyzer(
            redis_client=redis_client,
            db_session=db
        )
        
        # è·å–å‘é‡åŒ–å€™é€‰
        import asyncio
        candidates = asyncio.run(
            analyzer.prepare_vectorization_candidates(min_interaction_threshold=3)
        )
        
        if not candidates:
            logger.info("âœ“ æ— éœ€å‘é‡åŒ–çš„å€™é€‰")
            return {
                "status": "success",
                "vectorized_count": 0,
                "timestamp": datetime.now().isoformat()
            }
        
        # åˆå§‹åŒ–å‘é‡çŸ¥è¯†åº“
        from app.core.config import settings
        vector_kb = IntelligenceVectorKB(
            qdrant_host=settings.QDRANT_HOST,
            qdrant_port=settings.QDRANT_PORT,
            embedding_provider="qwen"
        )
        
        # å‘é‡åŒ–æ¯ä¸ªå€™é€‰
        vectorized_count = 0
        for candidate in candidates:
            report_data = candidate.get("report_data", {})
            report_id = candidate.get("report_id", "")
            
            # æå–å†…å®¹
            content = report_data.get("analysis", "")
            if len(content) < 20:  # å†…å®¹å¤ªçŸ­è·³è¿‡
                continue
            
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                "source": "multi_platform",
                "category": "intelligence_report",
                "sentiment": report_data.get("market_sentiment", "neutral"),
                "importance": candidate.get("priority", 0.5) / 10,  # å½’ä¸€åŒ–åˆ°0-1
                "timestamp": report_data.get("timestamp", datetime.now())
            }
            
            # å‘é‡åŒ–
            success = asyncio.run(
                vector_kb.vectorize_intelligence(
                    intelligence_id=report_id,
                    content=content,
                    metadata=metadata
                )
            )
            
            if success:
                vectorized_count += 1
        
        logger.info(f"âœ… æƒ…æŠ¥å‘é‡åŒ–å®Œæˆ: æˆåŠŸå‘é‡åŒ– {vectorized_count}/{len(candidates)} ä¸ªæƒ…æŠ¥")
        
        return {
            "status": "success",
            "candidates_count": len(candidates),
            "vectorized_count": vectorized_count,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ æƒ…æŠ¥å‘é‡åŒ–ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@celery_app.task(name='app.tasks.intelligence_learning.evaluate_intelligence_quality')
def evaluate_intelligence_quality():
    """
    æ¯æ—¥æƒ…æŠ¥è´¨é‡è¯„ä¼°
    
    èŒè´£ï¼š
    1. è¯„ä¼°æ˜¨æ—¥æƒ…æŠ¥çš„å‡†ç¡®æ€§
    2. ç»Ÿè®¡å„æºçš„æ•ˆæœ
    3. ç”Ÿæˆè´¨é‡æŠ¥å‘Š
    """
    try:
        logger.info("ğŸ“ˆ å¼€å§‹æ¯æ—¥æƒ…æŠ¥è´¨é‡è¯„ä¼°...")
        
        db = next(get_db())
        store = LongTermIntelligenceStore(db_session=db)
        
        # è·å–Topæº
        import asyncio
        top_sources = asyncio.run(
            store.get_top_sources(limit=20, metric="effectiveness")
        )
        
        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        quality_report = {
            "date": datetime.now().date().isoformat(),
            "top_sources": top_sources,
            "total_sources_evaluated": len(top_sources),
            "avg_effectiveness": (
                sum(s["effectiveness"] for s in top_sources) / len(top_sources)
                if top_sources else 0
            )
        }
        
        logger.info(
            f"âœ… æƒ…æŠ¥è´¨é‡è¯„ä¼°å®Œæˆ: "
            f"è¯„ä¼°{len(top_sources)}ä¸ªæº, "
            f"å¹³å‡æ•ˆæœ{quality_report['avg_effectiveness']:.2f}"
        )
        
        return {
            "status": "success",
            "report": quality_report,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ æƒ…æŠ¥è´¨é‡è¯„ä¼°ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@celery_app.task(name='app.tasks.intelligence_learning.analyze_patterns_weekly')
def analyze_patterns_weekly():
    """
    æ¯å‘¨æ¨¡å¼åˆ†æ
    
    èŒè´£ï¼š
    1. åˆ†æè¿‡å»7å¤©çš„æƒ…æŠ¥æ¨¡å¼
    2. è¯†åˆ«é‡å¤å‡ºç°çš„ä¸»é¢˜
    3. å‘ç°å¸‚åœºè¶‹åŠ¿
    """
    try:
        logger.info("ğŸ” å¼€å§‹æ¯å‘¨æ¨¡å¼åˆ†æ...")
        
        from app.core.config import settings
        vector_kb = IntelligenceVectorKB(
            qdrant_host=settings.QDRANT_HOST,
            qdrant_port=settings.QDRANT_PORT
        )
        
        # æŸ¥æ‰¾ä¸åŒç±»åˆ«çš„æ¨¡å¼
        categories = ["news", "whale", "onchain", "analysis"]
        all_patterns = []
        
        import asyncio
        for category in categories:
            patterns = asyncio.run(
                vector_kb.find_patterns(
                    category=category,
                    min_importance=0.6,
                    days=7
                )
            )
            all_patterns.extend(patterns)
        
        logger.info(f"âœ… æ¯å‘¨æ¨¡å¼åˆ†æå®Œæˆ: è¯†åˆ«åˆ° {len(all_patterns)} ä¸ªæ¨¡å¼")
        
        return {
            "status": "success",
            "patterns_found": len(all_patterns),
            "categories_analyzed": categories,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ æ¯å‘¨æ¨¡å¼åˆ†æä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@celery_app.task(name='app.tasks.intelligence_learning.generate_optimization_report')
def generate_optimization_report():
    """
    æ¯å‘¨ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    
    èŒè´£ï¼š
    1. æ±‡æ€»æœ¬å‘¨ä¼˜åŒ–æ•°æ®
    2. ç”Ÿæˆæ”¹è¿›å»ºè®®
    3. å‘é€æŠ¥å‘Šï¼ˆå¯é€‰ï¼‰
    """
    try:
        logger.info("ğŸ“ å¼€å§‹ç”Ÿæˆæ¯å‘¨ä¼˜åŒ–æŠ¥å‘Š...")
        
        db = next(get_db())
        optimizer = SourceWeightOptimizer(
            redis_client=redis_client,
            db_session=db
        )
        
        # è·å–ä¼˜åŒ–æŠ¥å‘Š
        import asyncio
        report = asyncio.run(
            optimizer.get_optimization_report()
        )
        
        # è·å–æ”¹è¿›å»ºè®®
        suggestions = asyncio.run(
            optimizer.suggest_improvements()
        )
        
        weekly_report = {
            "week_ending": datetime.now().date().isoformat(),
            "optimization_summary": report,
            "improvement_suggestions": suggestions,
            "total_suggestions": len(suggestions)
        }
        
        logger.info(
            f"âœ… æ¯å‘¨ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ: "
            f"{len(suggestions)} æ¡æ”¹è¿›å»ºè®®"
        )
        
        # TODO: å¯é€‰åœ°å‘é€æŠ¥å‘Šåˆ°ç®¡ç†å‘˜é‚®ç®±æˆ–Telegram
        
        return {
            "status": "success",
            "report": weekly_report,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Šä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


@celery_app.task(name='app.tasks.intelligence_learning.auto_generate_debate_report')
def auto_generate_debate_report():
    """
    æ¯4å°æ—¶è‡ªåŠ¨ç”Ÿæˆè¾©è®ºæŠ¥å‘Š
    
    èŒè´£ï¼š
    1. æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„æƒ…æŠ¥æŠ¥å‘Š
    2. è‡ªåŠ¨è§¦å‘è¾©è®ºç”Ÿæˆ
    3. ç¼“å­˜ç»“æœåˆ°Redis
    4. é¿å…ç”¨æˆ·æ¯æ¬¡éƒ½æ‰‹åŠ¨è§¦å‘
    
    ä¼˜åŠ¿ï¼š
    - ç”¨æˆ·è®¿é—®æ—¶ç›´æ¥ä»ç¼“å­˜è¯»å–ï¼Œæ— éœ€ç­‰å¾…
    - å®šæœŸæ›´æ–°ï¼Œä¿æŒæŠ¥å‘Šæ—¶æ•ˆæ€§
    - é™ä½APIè°ƒç”¨æˆæœ¬ï¼ˆé¿å…é‡å¤ç”Ÿæˆï¼‰
    """
    try:
        logger.info("ğŸ¤– å¼€å§‹è‡ªåŠ¨è¾©è®ºæŠ¥å‘Šç”Ÿæˆä»»åŠ¡...")
        
        # æ£€æŸ¥Redisç¼“å­˜æ˜¯å¦å·²è¿‡æœŸï¼ˆè¶…è¿‡30åˆ†é’Ÿï¼‰
        from app.core.redis_client import redis_client
        import asyncio
        
        async def check_and_generate():
            try:
                # æ£€æŸ¥ç¼“å­˜
                cached_report = await redis_client.get("debated_report:latest")
                
                if cached_report:
                    # æ£€æŸ¥ç¼“å­˜æ—¶é—´ï¼ˆé€šè¿‡TTLï¼‰
                    ttl = await redis_client.ttl("debated_report:latest")
                    remaining_time = ttl if ttl > 0 else 0
                    
                    # å¦‚æœç¼“å­˜å‰©ä½™æ—¶é—´ > 10åˆ†é’Ÿï¼Œè·³è¿‡ç”Ÿæˆ
                    if remaining_time > 600:  # 10åˆ†é’Ÿ
                        logger.info(f"â¸ï¸  ç¼“å­˜ä»ç„¶æœ‰æ•ˆï¼ˆå‰©ä½™{remaining_time//60}åˆ†é’Ÿï¼‰ï¼Œè·³è¿‡ç”Ÿæˆ")
                        return {
                            "status": "skipped",
                            "reason": "cache_still_valid",
                            "remaining_seconds": remaining_time
                        }
                
                # æ‰§è¡Œè¾©è®ºç”Ÿæˆ
                logger.info("ğŸ”„ ç¼“å­˜å·²è¿‡æœŸæˆ–ä¸å­˜åœ¨ï¼Œå¼€å§‹ç”Ÿæˆæ–°çš„è¾©è®ºæŠ¥å‘Š...")
                
                from app.core.database import AsyncSessionLocal
                from app.api.v1.intelligence import _execute_debate_and_cache
                
                async with AsyncSessionLocal() as db:
                    await _execute_debate_and_cache(db)
                
                logger.info("âœ… è‡ªåŠ¨è¾©è®ºæŠ¥å‘Šç”ŸæˆæˆåŠŸ")
                
                return {
                    "status": "success",
                    "message": "è¾©è®ºæŠ¥å‘Šå·²è‡ªåŠ¨ç”Ÿæˆå¹¶ç¼“å­˜",
                    "timestamp": datetime.now().isoformat()
                }
                
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨ç”Ÿæˆè¾©è®ºæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
                raise
        
        result = asyncio.run(check_and_generate())
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ è‡ªåŠ¨è¾©è®ºæŠ¥å‘Šä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }


# æ‰‹åŠ¨è§¦å‘ä»»åŠ¡çš„è¾…åŠ©å‡½æ•°
def trigger_weight_optimization():
    """æ‰‹åŠ¨è§¦å‘æƒé‡ä¼˜åŒ–"""
    return optimize_source_weights.delay()


def trigger_debate_generation():
    """æ‰‹åŠ¨è§¦å‘è¾©è®ºæŠ¥å‘Šç”Ÿæˆ"""
    return auto_generate_debate_report.delay()


def trigger_behavior_analysis():
    """æ‰‹åŠ¨è§¦å‘è¡Œä¸ºåˆ†æ"""
    return analyze_user_behavior.delay()


def trigger_vectorization():
    """æ‰‹åŠ¨è§¦å‘å‘é‡åŒ–"""
    return vectorize_daily_intelligence.delay()


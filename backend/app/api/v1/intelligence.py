"""Intelligence Report API Endpoints"""

import logging
from datetime import datetime, timedelta
from typing import List, Optional
from fastapi import APIRouter, Depends, HTTPException, Query, BackgroundTasks
from sqlalchemy.orm import Session
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import func, desc

from app.core.database import get_db
from app.models.intelligence import IntelligenceReport
from app.services.intelligence.storage import intelligence_storage

logger = logging.getLogger(__name__)

router = APIRouter()


@router.get("/latest")
async def get_latest_intelligence():
    """è·å–æœ€æ–°æƒ…æŠ¥ï¼ˆå¿«æ·è·¯å¾„ï¼‰"""
    try:
        report = await intelligence_storage.get_latest_report()
        if not report:
            raise HTTPException(status_code=404, detail="æš‚æ— æœ€æ–°æƒ…æŠ¥æŠ¥å‘Š")
        
        return {
            "success": True,
            "data": report.to_dict()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°æƒ…æŠ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/refresh")
async def refresh_intelligence(db: AsyncSession = Depends(get_db)):
    """æ‰‹åŠ¨è§¦å‘æƒ…æŠ¥æ”¶é›†ï¼ˆä½¿ç”¨æ–°çš„IntelligenceCoordinatorï¼‰"""
    try:
        # ä½¿ç”¨æ–°çš„ç»Ÿä¸€åè°ƒå™¨
        from app.services.intelligence.intelligence_coordinator import IntelligenceCoordinator
        from app.core.redis_client import redis_client
        
        logger.info("ğŸ”„ æ‰‹åŠ¨è§¦å‘æƒ…æŠ¥æ”¶é›†ï¼ˆä½¿ç”¨IntelligenceCoordinatorï¼‰...")
        
        # åˆ›å»ºåè°ƒå™¨å¹¶æ‰§è¡Œæ”¶é›†
        coordinator = IntelligenceCoordinator(redis_client, db)
        report = await coordinator.collect_intelligence()
        
        if not report:
            raise HTTPException(status_code=500, detail="æƒ…æŠ¥æ”¶é›†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        
        return {
            "success": True,
            "message": "æƒ…æŠ¥æ”¶é›†æˆåŠŸï¼ˆå¤šå¹³å°éªŒè¯+å››å±‚å­˜å‚¨ï¼‰",
            "data": report.to_dict()
        }
    
    except Exception as e:
        logger.error(f"æ‰‹åŠ¨æ”¶é›†æƒ…æŠ¥å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reports")
async def get_reports(
    limit: int = Query(default=20, ge=1, le=100, description="è¿”å›è®°å½•æ•°é‡"),
    db: AsyncSession = Depends(get_db)
):
    """
    è·å–æƒ…æŠ¥æŠ¥å‘Šåˆ—è¡¨ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    ç”¨äºå‰ç«¯é¡µé¢å¿«é€Ÿè·å–æœ€æ–°æŠ¥å‘Š
    """
    try:
        from sqlalchemy import select
        # æŸ¥è¯¢æœ€æ–°çš„æŠ¥å‘Š
        stmt = select(IntelligenceReport)\
                .order_by(desc(IntelligenceReport.timestamp))\
                .limit(limit)
        result = await db.execute(stmt)
        reports = result.scalars().all()
        
        return {
            "success": True,
            "data": [r.to_dict() for r in reports],
            "total": len(reports)
        }
    
    except Exception as e:
        logger.error(f"è·å–æƒ…æŠ¥æŠ¥å‘Šå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reports/latest")
async def get_latest_report():
    """è·å–æœ€æ–°çš„æƒ…æŠ¥æŠ¥å‘Šï¼ˆæ¥è‡ªRedisç¼“å­˜ï¼‰"""
    try:
        report = await intelligence_storage.get_latest_report()
        if not report:
            raise HTTPException(status_code=404, detail="æš‚æ— æœ€æ–°æƒ…æŠ¥æŠ¥å‘Š")
        
        return {
            "success": True,
            "data": report.to_dict()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æœ€æ–°æƒ…æŠ¥æŠ¥å‘Šå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reports/history")
async def get_report_history(
    limit: int = Query(default=10, ge=1, le=100, description="è¿”å›è®°å½•æ•°é‡"),
    offset: int = Query(default=0, ge=0, description="åç§»é‡"),
    start_date: Optional[datetime] = Query(default=None, description="èµ·å§‹æ—¶é—´"),
    end_date: Optional[datetime] = Query(default=None, description="ç»“æŸæ—¶é—´"),
    sentiment: Optional[str] = Query(default=None, description="æƒ…ç»ªç±»å‹: BULLISH/BEARISH/NEUTRAL"),
    min_confidence: Optional[float] = Query(default=None, ge=0.0, le=1.0, description="æœ€å°ç½®ä¿¡åº¦"),
    db: Session = Depends(get_db)
):
    """
    è·å–å†å²æƒ…æŠ¥æŠ¥å‘Šï¼ˆæ¥è‡ªPostgreSQLæŒä¹…åŒ–å­˜å‚¨ï¼‰
    
    æ”¯æŒå¤šç§ç­›é€‰æ¡ä»¶ï¼š
    - æ—¶é—´èŒƒå›´
    - æƒ…ç»ªç±»å‹
    - æœ€å°ç½®ä¿¡åº¦
    """
    try:
        # æ„å»ºæŸ¥è¯¢
        query = db.query(IntelligenceReport)
        
        # æ—¶é—´èŒƒå›´ç­›é€‰
        if start_date:
            query = query.filter(IntelligenceReport.timestamp >= start_date)
        if end_date:
            query = query.filter(IntelligenceReport.timestamp <= end_date)
        
        # æƒ…ç»ªç±»å‹ç­›é€‰
        if sentiment:
            query = query.filter(IntelligenceReport.market_sentiment == sentiment.upper())
        
        # ç½®ä¿¡åº¦ç­›é€‰
        if min_confidence is not None:
            query = query.filter(IntelligenceReport.confidence >= min_confidence)
        
        # æ’åºå’Œåˆ†é¡µ
        total = query.count()
        reports = query.order_by(desc(IntelligenceReport.timestamp))\
                      .offset(offset)\
                      .limit(limit)\
                      .all()
        
        return {
            "success": True,
            "data": {
                "total": total,
                "limit": limit,
                "offset": offset,
                "reports": [r.to_dict() for r in reports]
            }
        }
    
    except Exception as e:
        logger.error(f"è·å–å†å²æƒ…æŠ¥æŠ¥å‘Šå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/reports/{report_id}")
async def get_report_by_id(
    report_id: int,
    db: Session = Depends(get_db)
):
    """æ ¹æ®IDè·å–ç‰¹å®šæƒ…æŠ¥æŠ¥å‘Š"""
    try:
        report = db.query(IntelligenceReport).filter(
            IntelligenceReport.id == report_id
        ).first()
        
        if not report:
            raise HTTPException(status_code=404, detail=f"æƒ…æŠ¥æŠ¥å‘Šä¸å­˜åœ¨: ID={report_id}")
        
        return {
            "success": True,
            "data": report.to_dict()
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æƒ…æŠ¥æŠ¥å‘Šå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analytics/summary")
async def get_analytics_summary(
    days: int = Query(default=7, ge=1, le=90, description="ç»Ÿè®¡å¤©æ•°"),
    db: Session = Depends(get_db)
):
    """
    è·å–æƒ…æŠ¥åˆ†æç»Ÿè®¡æ‘˜è¦
    
    åŒ…æ‹¬ï¼š
    - æ€»æŠ¥å‘Šæ•°
    - æƒ…ç»ªåˆ†å¸ƒ
    - å¹³å‡ç½®ä¿¡åº¦
    - è¶‹åŠ¿åˆ†æ
    """
    try:
        start_date = datetime.now() - timedelta(days=days)
        
        # åŸºç¡€ç»Ÿè®¡
        total_reports = db.query(func.count(IntelligenceReport.id))\
            .filter(IntelligenceReport.timestamp >= start_date)\
            .scalar()
        
        # æƒ…ç»ªåˆ†å¸ƒ
        sentiment_stats = db.query(
            IntelligenceReport.market_sentiment,
            func.count(IntelligenceReport.id).label('count')
        ).filter(
            IntelligenceReport.timestamp >= start_date
        ).group_by(
            IntelligenceReport.market_sentiment
        ).all()
        
        sentiment_distribution = {
            s[0]: s[1] for s in sentiment_stats
        }
        
        # å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = db.query(
            func.avg(IntelligenceReport.confidence)
        ).filter(
            IntelligenceReport.timestamp >= start_date
        ).scalar()
        
        # æ¯æ—¥æŠ¥å‘Šæ•°
        daily_reports = db.query(
            func.date(IntelligenceReport.timestamp).label('date'),
            func.count(IntelligenceReport.id).label('count')
        ).filter(
            IntelligenceReport.timestamp >= start_date
        ).group_by(
            func.date(IntelligenceReport.timestamp)
        ).order_by(
            desc('date')
        ).all()
        
        return {
            "success": True,
            "data": {
                "period_days": days,
                "start_date": start_date.isoformat(),
                "end_date": datetime.now().isoformat(),
                "total_reports": total_reports or 0,
                "sentiment_distribution": sentiment_distribution,
                "average_confidence": float(avg_confidence) if avg_confidence else 0.0,
                "daily_reports": [
                    {
                        "date": str(d[0]),
                        "count": d[1]
                    }
                    for d in daily_reports
                ]
            }
        }
    
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡æ‘˜è¦å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analytics/sentiment-trend")
async def get_sentiment_trend(
    days: int = Query(default=30, ge=1, le=90, description="ç»Ÿè®¡å¤©æ•°"),
    db: Session = Depends(get_db)
):
    """
    è·å–å¸‚åœºæƒ…ç»ªè¶‹åŠ¿
    
    è¿”å›æ¯æ—¥çš„æƒ…ç»ªåˆ†å¸ƒï¼Œç”¨äºè¶‹åŠ¿åˆ†æ
    """
    try:
        start_date = datetime.now() - timedelta(days=days)
        
        # æŸ¥è¯¢æ¯æ—¥å„æƒ…ç»ªçš„æ•°é‡
        trend_data = db.query(
            func.date(IntelligenceReport.timestamp).label('date'),
            IntelligenceReport.market_sentiment,
            func.count(IntelligenceReport.id).label('count'),
            func.avg(IntelligenceReport.sentiment_score).label('avg_score')
        ).filter(
            IntelligenceReport.timestamp >= start_date
        ).group_by(
            func.date(IntelligenceReport.timestamp),
            IntelligenceReport.market_sentiment
        ).order_by(
            'date'
        ).all()
        
        # ç»„ç»‡æ•°æ®
        daily_data = {}
        for row in trend_data:
            date_str = str(row[0])
            if date_str not in daily_data:
                daily_data[date_str] = {
                    "date": date_str,
                    "sentiments": {},
                    "scores": {}
                }
            daily_data[date_str]["sentiments"][row[1]] = row[2]
            daily_data[date_str]["scores"][row[1]] = float(row[3]) if row[3] else 0.0
        
        return {
            "success": True,
            "data": {
                "period_days": days,
                "trend": list(daily_data.values())
            }
        }
    
    except Exception as e:
        logger.error(f"è·å–æƒ…ç»ªè¶‹åŠ¿å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/analytics/data-quality")
async def get_data_quality_stats(
    days: int = Query(default=7, ge=1, le=30, description="ç»Ÿè®¡å¤©æ•°"),
    db: Session = Depends(get_db)
):
    """
    è·å–æ•°æ®è´¨é‡ç»Ÿè®¡
    
    åŒ…æ‹¬ï¼š
    - å„æ•°æ®æºçš„è¦†ç›–ç‡
    - ç½®ä¿¡åº¦åˆ†å¸ƒ
    - æ•°æ®å®Œæ•´æ€§
    """
    try:
        start_date = datetime.now() - timedelta(days=days)
        
        reports = db.query(IntelligenceReport).filter(
            IntelligenceReport.timestamp >= start_date
        ).all()
        
        if not reports:
            return {
                "success": True,
                "data": {
                    "period_days": days,
                    "total_reports": 0,
                    "data_coverage": {},
                    "confidence_distribution": {},
                    "completeness": 0.0
                }
            }
        
        # æ•°æ®è¦†ç›–ç‡
        coverage = {
            "news": sum(1 for r in reports if r.key_news and len(r.key_news) > 0),
            "whale": sum(1 for r in reports if r.whale_signals and len(r.whale_signals) > 0),
            "onchain": sum(1 for r in reports if r.on_chain_metrics),
        }
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_ranges = {
            "0.0-0.3": 0,
            "0.3-0.5": 0,
            "0.5-0.7": 0,
            "0.7-0.9": 0,
            "0.9-1.0": 0
        }
        
        for r in reports:
            conf = r.confidence
            if conf < 0.3:
                confidence_ranges["0.0-0.3"] += 1
            elif conf < 0.5:
                confidence_ranges["0.3-0.5"] += 1
            elif conf < 0.7:
                confidence_ranges["0.5-0.7"] += 1
            elif conf < 0.9:
                confidence_ranges["0.7-0.9"] += 1
            else:
                confidence_ranges["0.9-1.0"] += 1
        
        # å®Œæ•´æ€§è¯„åˆ†ï¼ˆæœ‰åˆ†ææ–‡æœ¬çš„æ¯”ä¾‹ï¼‰
        complete_reports = sum(
            1 for r in reports 
            if r.qwen_analysis and len(r.qwen_analysis) > 0
        )
        completeness = complete_reports / len(reports) if reports else 0.0
        
        return {
            "success": True,
            "data": {
                "period_days": days,
                "total_reports": len(reports),
                "data_coverage": {
                    "news_coverage": coverage["news"] / len(reports),
                    "whale_coverage": coverage["whale"] / len(reports),
                    "onchain_coverage": coverage["onchain"] / len(reports),
                },
                "confidence_distribution": confidence_ranges,
                "completeness_score": completeness
            }
        }
    
    except Exception as e:
        logger.error(f"è·å–æ•°æ®è´¨é‡ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/debated-report")
async def get_debated_intelligence_report(db: AsyncSession = Depends(get_db)):
    """
    è·å–ç»è¿‡è¾©è®ºéªŒè¯çš„æƒ…æŠ¥æŠ¥å‘Šï¼ˆä»ç¼“å­˜è¯»å–ï¼‰
    
    æµç¨‹ï¼š
    1. ä¼˜å…ˆä» Redis ç¼“å­˜è¯»å–æœ€æ–°çš„è¾©è®ºæŠ¥å‘Š
    2. å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œè¿”å›æœ€æ–°çš„ Qwen æƒ…æŠ¥ï¼ˆæœªè¾©è®ºï¼‰
    3. å»ºè®®ï¼šä½¿ç”¨ POST /trigger-debate åœ¨åå°ç”Ÿæˆæ–°çš„è¾©è®ºæŠ¥å‘Š
    """
    try:
        from app.core.redis_client import redis_client
        import json
        
        logger.info("ğŸ”„ è·å–è¾©è®ºæŠ¥å‘Šï¼ˆä»ç¼“å­˜ï¼‰...")
        
        # 1. å°è¯•ä» Redis ç¼“å­˜è·å–
        cached_report = await redis_client.get("debated_report:latest")
        
        if cached_report:
            try:
                # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦åˆ™è§£æJSON
                if isinstance(cached_report, dict):
                    debate_data = cached_report
                elif isinstance(cached_report, str):
                    debate_data = json.loads(cached_report)
                else:
                    # å°è¯•è§£ç ä¸ºå­—ç¬¦ä¸²åå†è§£æ
                    debate_data = json.loads(str(cached_report))
                
                logger.info("âœ… ä»ç¼“å­˜è¿”å›è¾©è®ºæŠ¥å‘Š")
                return {
                    "success": True,
                    "data": debate_data,
                    "cached": True
                }
            except (json.JSONDecodeError, TypeError, ValueError) as e:
                logger.warning(f"ç¼“å­˜è§£æå¤±è´¥: {e}, ç±»å‹: {type(cached_report)}")
        
        # 2. ç¼“å­˜ä¸å­˜åœ¨ï¼Œè¿”å›åŸå§‹æƒ…æŠ¥ + æç¤º
        logger.info("âš ï¸  ç¼“å­˜ä¸­æ— è¾©è®ºæŠ¥å‘Šï¼Œè¿”å›åŸå§‹æƒ…æŠ¥")
        report = await intelligence_storage.get_latest_report()
        if not report:
            raise HTTPException(status_code=404, detail="æš‚æ— æœ€æ–°æƒ…æŠ¥æŠ¥å‘Š")
        
        # å‡†å¤‡æƒ…æŠ¥å­—å…¸
        intelligence_dict = {
            "market_sentiment": report.market_sentiment.value if hasattr(report.market_sentiment, 'value') else str(report.market_sentiment),
            "confidence": report.confidence,
            "summary": report.qwen_analysis[:500] if report.qwen_analysis else "",
            "key_news": report.key_news[:3] if report.key_news else [],
            "whale_signals": report.whale_signals[:3] if report.whale_signals else [],
            "platform_contributions": getattr(report, 'platform_contributions', {}),
            "platform_consensus": getattr(report, 'platform_consensus', 0.0),
        }
        
        return {
            "success": True,
            "data": {
                "original_intelligence": {
                    "market_sentiment": intelligence_dict["market_sentiment"],
                    "confidence": intelligence_dict["confidence"],
                    "summary": intelligence_dict["summary"],
                    "key_news": intelligence_dict["key_news"],
                    "whale_signals": intelligence_dict["whale_signals"],
                    "timestamp": report.timestamp.isoformat() if report.timestamp else None
                },
                "debate_result": None,  # æ— è¾©è®ºç»“æœ
                "enhanced_sentiment": intelligence_dict["market_sentiment"],
                "enhanced_confidence": intelligence_dict["confidence"],
                "is_debated": False
            },
            "cached": False,
            "message": "è¾©è®ºæŠ¥å‘Šæœªç”Ÿæˆï¼Œè¯·è°ƒç”¨ POST /trigger-debate ç”Ÿæˆ"
        }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–è¾©è®ºæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–å¤±è´¥: {str(e)}")


async def _execute_debate_and_cache(db: AsyncSession):
    """
    æ‰§è¡Œè¾©è®ºå¹¶å°†ç»“æœç¼“å­˜åˆ° Redisï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
    """
    from app.services.decision.debate_system import DebateCoordinator
    from app.services.decision.prompt_manager_db import PromptManagerDB
    from app.core.redis_client import redis_client
    import openai
    import json
    from app.core.config import settings
    
    try:
        logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œè¾©è®º...")
        
        # 1. è·å–æœ€æ–°çš„ Qwen æƒ…æŠ¥
        report = await intelligence_storage.get_latest_report()
        if not report:
            raise ValueError("æš‚æ— æœ€æ–°æƒ…æŠ¥æŠ¥å‘Š")
        
        logger.info(f"ğŸ“Š è·å–åˆ° Qwen æƒ…æŠ¥: æƒ…ç»ª={report.market_sentiment}, ç½®ä¿¡åº¦={report.confidence:.2%}")
        
        # 2. å‡†å¤‡å¸‚åœºæ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼Œç”¨äºè¾©è®ºï¼‰
        market_data = {
            "BTC": {
                "price": 95000,  # å¯ä»¥ä»å®é™…å¸‚åœºæ•°æ®è·å–
                "change_24h": 2.5
            }
        }
        
        # 3. å‡†å¤‡æƒ…æŠ¥å­—å…¸ï¼ˆç¡®ä¿æ‰€æœ‰å¯¹è±¡éƒ½å¯åºåˆ—åŒ–ï¼‰
        def serialize_news_item(item):
            """å°† NewsItem å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
            if isinstance(item, dict):
                return item
            return {
                "title": getattr(item, 'title', ''),
                "source": getattr(item, 'source', ''),
                "url": getattr(item, 'url', ''),
                "published_at": str(getattr(item, 'published_at', '')),
                "content": getattr(item, 'content', ''),
                "impact": getattr(item, 'impact', 'medium'),
                "sentiment": getattr(item, 'sentiment', 'neutral')
            }
        
        def serialize_whale_signal(item):
            """å°† WhaleSignal å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
            if isinstance(item, dict):
                return item
            return {
                "symbol": getattr(item, 'symbol', ''),
                "action": getattr(item, 'action', ''),
                "amount_usd": float(getattr(item, 'amount_usd', 0)),
                "address": getattr(item, 'address', ''),
                "timestamp": str(getattr(item, 'timestamp', '')),
                "exchange": getattr(item, 'exchange', None)
            }
        
        intelligence_dict = {
            "market_sentiment": report.market_sentiment.value if hasattr(report.market_sentiment, 'value') else str(report.market_sentiment),
            "confidence": float(report.confidence) if report.confidence else 0.0,
            "summary": report.qwen_analysis[:500] if report.qwen_analysis else "",
            "key_news": [serialize_news_item(item) for item in (report.key_news[:3] if report.key_news else [])],
            "whale_signals": [serialize_whale_signal(item) for item in (report.whale_signals[:3] if report.whale_signals else [])],
            "platform_contributions": getattr(report, 'platform_contributions', {}),
            "platform_consensus": float(getattr(report, 'platform_consensus', 0.0)),
        }
        
        # 4. åˆå§‹åŒ–è¾©è®ºç³»ç»Ÿ
        llm_client = openai.OpenAI(
            api_key=settings.DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com/v1"
        )
        
        prompt_manager = PromptManagerDB(db)
        
        debate_coordinator = DebateCoordinator(
            llm_client=llm_client,
            max_debate_rounds=1,  # 1è½®è¾©è®º
            timeout_seconds=60,
            prompt_manager=prompt_manager
        )
        
        # 5. æ‰§è¡Œè¾©è®º
        logger.info("âš”ï¸  å¯åŠ¨å¤šç©ºè¾©è®º...")
        debate_result = await debate_coordinator.conduct_debate(
            market_data=market_data,
            intelligence_report=intelligence_dict,
            past_memories=[]
        )
        
        logger.info(f"âœ… è¾©è®ºå®Œæˆ: æ¨è={debate_result['final_decision'].get('recommendation')}, "
                   f"å…±è¯†åº¦={debate_result['consensus_level']:.2f}")
        
        # 6. æ„å»ºç»“æœæ•°æ®ï¼ˆç¡®ä¿æ‰€æœ‰æ•°å€¼å¯åºåˆ—åŒ–ï¼‰
        result_data = {
            # åŸå§‹ Qwen æƒ…æŠ¥
            "original_intelligence": {
                "market_sentiment": intelligence_dict["market_sentiment"],
                "confidence": intelligence_dict["confidence"],
                "summary": intelligence_dict["summary"],
                "key_news": intelligence_dict["key_news"],
                "whale_signals": intelligence_dict["whale_signals"],
                "timestamp": report.timestamp.isoformat() if report.timestamp else None
            },
            # è¾©è®ºç»“æœ
            "debate_result": {
                "recommendation": str(debate_result['final_decision'].get('recommendation', 'HOLD')),
                "confidence": float(debate_result['final_decision'].get('confidence', 0.5)),
                "reasoning": str(debate_result['final_decision'].get('reasoning', '')),
                "bull_argument": debate_result['debate_history'].get('bull_arguments', []),
                "bear_argument": debate_result['debate_history'].get('bear_arguments', []),
                "consensus_level": float(debate_result.get('consensus_level', 0.0)),
                "total_rounds": int(debate_result.get('total_rounds', 0)),
                "duration_seconds": float(debate_result.get('duration_seconds', 0.0))
            },
            # ç»¼åˆåˆ†æ
            "enhanced_sentiment": str(debate_result['final_decision'].get('recommendation', 'HOLD')),
            "enhanced_confidence": float(debate_result['final_decision'].get('confidence', 0.5)),
            "is_debated": True
        }
        
        # 7. ä¿å­˜åˆ° Redis ç¼“å­˜ï¼ˆ30åˆ†é’Ÿè¿‡æœŸï¼‰
        await redis_client.setex(
            "debated_report:latest",
            1800,  # 30åˆ†é’Ÿ
            json.dumps(result_data, ensure_ascii=False)
        )
        
        logger.info("ğŸ’¾ è¾©è®ºç»“æœå·²ç¼“å­˜åˆ° Redis")
        return result_data
        
    except Exception as e:
        logger.error(f"è¾©è®ºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        raise


@router.post("/trigger-debate")
async def trigger_debate_manually(background_tasks: BackgroundTasks, db: AsyncSession = Depends(get_db)):
    """
    æ‰‹åŠ¨è§¦å‘æƒ…æŠ¥è¾©è®ºï¼ˆå¼‚æ­¥åå°æ‰§è¡Œï¼‰
    
    ç”¨æˆ·å¯ä»¥ç‚¹å‡»æŒ‰é’®æ‰‹åŠ¨è§¦å‘æ–°ä¸€è½®è¾©è®º
    è¾©è®ºå°†åœ¨åå°æ‰§è¡Œï¼Œé¿å…è¶…æ—¶
    """
    try:
        import asyncio
        
        logger.info("ğŸš€ å¯åŠ¨åå°è¾©è®ºä»»åŠ¡...")
        
        # åœ¨åå°æ‰§è¡Œè¾©è®º
        async def run_debate_in_background():
            try:
                # åˆ›å»ºæ–°çš„æ•°æ®åº“ä¼šè¯
                from app.core.database import AsyncSessionLocal
                async with AsyncSessionLocal() as bg_db:
                    await _execute_debate_and_cache(bg_db)
                    logger.info("âœ… åå°è¾©è®ºä»»åŠ¡å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ åå°è¾©è®ºä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        
        # å¯åŠ¨åå°ä»»åŠ¡
        asyncio.create_task(run_debate_in_background())
        
        return {
            "success": True,
            "message": "è¾©è®ºå·²å¯åŠ¨ï¼Œæ­£åœ¨åå°æ‰§è¡Œä¸­...",
            "status": "processing",
            "estimated_time": "60-90ç§’"
        }
    
    except Exception as e:
        logger.error(f"å¯åŠ¨è¾©è®ºä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))
